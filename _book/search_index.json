[
["index.html", "Introduction to Statistical Methodology Chapter 1 Summary Statistics and Graphing 1.1 Graphical summaries of data 1.2 Measures of Centrality 1.3 Measures of Variation 1.4 Exercises", " Introduction to Statistical Methodology Derek L. Sonderegger 2017-01-24 Chapter 1 Summary Statistics and Graphing When confronted with a large amount of data, we seek to summarize the data into statistics that somehow capture the essence of the data with as few numbers as possible. Graphing the data has a similar goal… to reduce the data to an image that represents all the key aspects of the raw data. In short, we seek to simplify the data in order to understand the trends while not obscuring important structure. # Every chapter, we will load all the librarys we will use at the beginning # of the chapter. If you library(mosaicData) # library of datasets we&#39;ll use library(ggplot2) # graphing functions library(dplyr) # data summary tools For this chapter, we will consider data from a the 2005 Cherry Blossom 10 mile run that occurs in Washington DC. This data set has 8636 observations that includes the runners state of residence, official time (gun to finish, in seconds), net time (start line to finish, in seconds), age, and gender of the runners. head(TenMileRace) # examine the first few rows of the data ## state time net age sex ## 1 VA 6060 5978 12 M ## 2 MD 4515 4457 13 M ## 3 VA 5026 4928 13 M ## 4 MD 4229 4229 14 M ## 5 MD 5293 5076 14 M ## 6 VA 6234 5968 14 M In general, I often need to make a distinction between two types of data. Discrete (also called Categorical) data is data that can only take a small set of particular values. For example a college student’s grade can be either A, B, C, D, or F. A person’s sex can be only Male or Female.Actually this isn’t true as both gender and sex are far more complex. However from a statistical point of view it is often useful to simplify our model of the world. George Box famously said, “All models are wrong, but some are useful.” Discrete data could also be numeric, for example a bird could lay 1, 2, 3, … eggs in a breeding season. Continuous data is data that can take on an infinite number of numerical values. For example a person’s height could be 68 inches, 68.2 inches, 68.23212 inches. To decided if a data attribute is discrete or continuous, I often as “Does a fraction of a value make sense?” If so, then the data is continuous. 1.1 Graphical summaries of data 1.1.1 Univariate - Categorical If we have univariate data about a number of groups, often the best way to display it is using barplots. They have the advantage over pie-charts that groups are easily compared. ggplot(TenMileRace, aes(x=sex)) + geom_bar() One thing that can be misleading is if the zero on the y-axis is removed. In the following graph it looks like there are twice as many female runners as male until you examin the y-axis closely. In general, the following is a very misleading graph. ggplot(TenMileRace, aes(x=sex)) + geom_bar() + coord_cartesian(ylim = c(4300, 4330)) 1.1.2 Univariate - Continuous A histogram looks very similar to a bar plot, but is used to represent continuous data instead of categorical and therefore the bars will actually be touching. ggplot(TenMileRace, aes(x=net)) + geom_histogram() Often when a histogram is presented, the y-axis is labeled as “frequency” or “count” which is the number of observations that fall within a particular bin. However, it is often desirable to scale the y-axis so that if we were to sum up the area \\((height * width)\\) then the total area would sum to 1. The rescaling that accomplishes this is \\[density=\\frac{\\#\\;observations\\;in\\;bin}{total\\;number\\;observations}\\cdot\\frac{1}{bin\\;width}\\] 1.1.3 Bivariate - Categorical vs Continuous We often wish to compare response levels from two or more groups of interest. To do this, we often use side-by-side boxplots. Notice that each observation is associated with a continuous response value and a categorical value. ggplot(TenMileRace, aes(x=sex, y=net)) + geom_boxplot() In this graph, the edges of the box are defined by the 25% and 75% quantiles. That is to say, 25% of the data is to the below of the box, 50% of the data is in the box, and the final 25% of the data is to the above of the box. The dots are data points that traditionally considered outliers.Define the Inter-Quartile Range (IQR) as the length of the box. Then any observation more than 1.5*IQR from the box is considered an outlier. Sometimes I think that box-and-whisker plot obscures too much of the details of the data and we should look at the side-by-side histograms instead. ggplot(TenMileRace, aes(x=net)) + geom_histogram() + facet_grid( . ~ sex ) # side-by-side plots based on sex Orientation of graphs can certainly matter. In this case, it makes sense to stack the two graphs to facilitate comparisons. ggplot(TenMileRace, aes(x=net)) + geom_histogram() + facet_grid( sex ~ . ) # side-by-side plots based on sex ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 1.1.4 Bivariate - Continuous vs Continuous Finally we might want to examine the relationship between two continuous random variables. ggplot(TenMileRace, aes(x=age, y=net, color=sex)) + geom_point() 1.2 Measures of Centrality The most basic question to ask of any dataset is ‘What is the typical value?’ There are several ways to answer that question and they should be familiar to most students. 1.2.1 Mean Often called the average, or arithmetic mean, we will denote this special statistic with a bar. We define \\[\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}=\\frac{1}{n}\\left(x_{1}+x_{2}+\\dots+x_{n}\\right)\\] If we want to find the mean of five numbers \\(\\left\\{ 3,6,4,8,2\\right\\}\\) the calculation is \\[\\bar{x} = \\frac{1}{5}\\left(3+6+4+8+2\\right) = \\frac{1}{5}\\left(23\\right) = 23/5 = 4.6\\] This can easily be calculated in R by using the function mean(). We first extract the column we are interested in using the notation: DataSet$ColumnName where the $ signifies grabbing the column. mean( TenMileRace$net ) # Simplest way of doing this calculation ## [1] 5599.065 TenMileRace %&gt;% summarise( mean(net) ) # using the dplyr package ## mean(net) ## 1 5599.065 1.2.2 Median If the data were to be ordered, the median would be the middle most observation (or, in the case that \\(n\\) is even, the mean of the two middle most values). In our simple case of five observations \\(\\left\\{ 3,6,4,8,2\\right\\}\\), we first sort the data into \\(\\left\\{ 2,3,4,6,8\\right\\}\\) and then the middle observation is clearly \\(4\\). In R the median is easily calculated by the function median(). # median( TenMileRace$net ) TenMileRace %&gt;% summarise( median(net) ) ## median(net) ## 1 5555 1.2.3 Mode This is the observation value with the most number of occurrences. This measure of “center” is not often used 1.2.4 Examples If my father were to become bored with retirement and enroll in my STA 570 course, how would that affect the mean and median age of my 570 students? The mean would move much more than the median. Suppose the class has 5 people right now, ages 21, 22, 23, 23, 24 and therefore the median is 23. When my father joins, the ages will be 21, 22, 23, 23, 24, 72 and the median will remain 23. However, the mean would move because we add in such a large outlier. Whenever we are dealing with skewed data, the mean is pulled toward the outlying observations. In 2010, the median NFL player salary was $770,000 while the mean salary was $1.9 million. Why the difference? Because salary data is skewed superstar players that make huge salaries (in excess of 20 million) while the minimum salary for a rookie is $375,000. Financial data often reflects a highly skewed distribution and the median is often a better measure of centrality in these cases. 1.3 Measures of Variation The second question to ask of a dataset is ‘How much variability is there?’ Again there are several ways to measure that. 1.3.1 Range Range is the distance from the largest to the smallest value in the dataset. #vmax( TenMileRace$net ) - min( TenMileRace$net ) TenMileRace %&gt;% summarise( range = max(net) - min(net) ) ## range ## 1 7722 1.3.2 Inter-Quartile Range The p-th percentile is the observation (or observations) that has at most \\(p\\) percent of the observations below it and \\((1-p)\\) above it, where \\(p\\) is between 0 and 100. The median is the \\(50\\)th percentile. Often we are interested in splitting the data into four equal sections using the \\(25\\)th, \\(50\\)th, and \\(75\\)th percentiles (which, because it splits the data into four sections, we often call these the \\(1\\)st, \\(2\\)nd, and \\(3\\)rd quartiles). In general I could be interested in dividing my data up into an arbitrary number of sections, and refer to those as quantiles of my data. quantile( TenMileRace$net ) # this works ## 0% 25% 50% 75% 100% ## 2814 4950 5555 6169 10536 # I can&#39;t do the following because the quantile() function spits out 5 values, not 1 # TenMileRace %&gt;% summarise( quantile(net) ) The inter-quartile range (IQR) is defined as the distance from the \\(3\\)rd quartile to the \\(1\\)st. # IQR( TenMileRace$net ) TenMileRace %&gt;% summarise( IQR(net) ) ## IQR(net) ## 1 1219 Notice that we’ve defined IQR before when we looked at box-and-whisker plots and this is exactly the length of the box. 1.3.3 Variance One way to measure the spread of a distribution is to ask “what is the typical distance of an observation to the mean?” We could define the \\(i\\)th deviate as \\[e_{i}=x_{i}-\\bar{x}\\] and then ask what is the average deviate? The problem with this approach is that the sum (and thus the average) of all deviates is always 0. \\[\\sum_{i=1}^{n}(x_{i}-\\bar{x}) = \\sum_{i=1}^{n}x_{i}-\\sum_{i=1}^{n}\\bar{x} = n\\frac{1}{n}\\sum_{i=1}^{n}x_{i}-n\\bar{x} = n\\bar{x}-n\\bar{x} = 0\\] The big problem is that about half the deviates are negative and the others are positive. What we really care is the distance from the mean, not the sign. So we could either take the absolute value, or square it. There are some really good theoretical reasons to chose the square option. Squared terms are easier to deal with compared to absolute values, but more importantly, the spread of the normal distribution is parameterized via squared distances from the mean. Because the normal distribution is so important, we’ve chosen to define the sample variance so it matches up with the natural spread parameter of the normal distribution. So we square the deviates and then find the average deviate size (approximately) and call that the sample variance. \\[s^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}\\] Why do we divide by \\(n-1\\) instead of \\(n\\)? If I divide by \\(n\\), then on average, we would tend to underestimate the population variance \\(\\sigma^{2}\\). The reason is because we are using the same set of data to estimate \\(\\sigma^{2}\\) as we did to estimate the population mean (\\(\\mu\\)). If I could use \\[\\frac{1}{n}\\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)^{2}\\] as my estimator, we would be fine. But because I have to replace \\(\\mu\\) with \\(\\bar{x}\\) we have to pay a price. Because the estimation of \\(\\sigma^{2}\\) requires the estimation of one other quantity, and using using that quantity, you only need \\(n-1\\) data points and can then figure out the last one, we have used one degree of freedom on estimating the mean and we need to adjust the formula accordingly. In later chapters we’ll give this quantity a different name, so we’ll introduce the necessary vocabulary here. Let \\(e_{i}=x_{i}-\\bar{x}\\) be the error left after fitting the sample mean. This is the deviation from the observed value to the “expected value” \\(\\bar{x}\\). We can then define the Sum of Squared Error as \\[SSE=\\sum_{i=1}^{n}e_{i}^{2}\\] and the Mean Squared Error as \\[MSE=\\frac{SSE}{df}=\\frac{SSE}{n-1}=s^{2}\\] where \\(df=n-1\\) is the appropriate degrees of freedom. Calculating the variance of our small sample of five observations \\(\\left\\{ 3,6,4,8,2\\right\\}\\), recall that the sample mean was \\(\\bar{x}=4.6\\) \\(x_i\\) \\((x_i-\\bar{x})\\) \\((x_i-\\bar{x})^2\\) 3 -1.6 2.56 6 1.4 1.96 4 -0.6 0.36 8 3.4 11.56 2 -2.6 6.76 SSE = 23.2 and so the sample variance is \\[s^2 = \\frac{SSE}{n-1} = \\frac{23.2}{(n-1)} = \\frac{23.2}{4}=5.8\\] Clearly this calculation would get very tedious to do by hand and computers will be much more accurate in these calculations. In R, the sample variance is easily calculated by the function var(). ToyData &lt;- data.frame( x=c(3,6,4,8,2) ) # var( ToyData$x ) ToyData %&gt;% summarise( s2 = var(x) ) ## s2 ## 1 5.8 For the larger TenMileRace data set, the variance is just as easily calculated. # var( TenMileRace$net ) TenMileRace %&gt;% summarise( s2 = var(net) ) ## s2 ## 1 940233.5 1.3.4 Standard Deviation The biggest problem with the sample variance statistic is that the units are in the original units-squared. That means if you are looking at data about car fuel efficiency, then the values would be in mpg\\(^{2}\\) which are units that I can’t really understand. The solution is to take the positive square root, which we will call the sample standard deviation. \\[s=\\sqrt{s^{2}}\\] But why do we take the jog through through variance? Mathematically the variance is more useful and most distributions (such as the normal) are defined by the variance term. Practically though, standard deviation is easier to think about. The sample standard deviation is important enough for R to have function that will calculate it for you. # sd( TenMileRace$net ) TenMileRace %&gt;% summarise( s = sd(net) ) ## s ## 1 969.6564 1.3.5 Coefficient of Variation Suppose we had a group of animals and the sample standard deviation of the animals lengths was 15 cm. If the animals were elephants, you would be amazed at their uniformity in size, but if they were insects, you would be astounded at the variability. To account for that, the coefficient of variation takes the sample standard deviation and divides by the absolute value of the sample mean (to keep everything positive) \\[CV=\\frac{s}{\\vert\\bar{x}\\vert}\\] TenMileRace %&gt;% summarise( s = sd(net), xbar = mean(net), cv = s / abs(xbar) ) ## s xbar cv ## 1 969.6564 5599.065 0.1731818 # For fun, lets calculate these same statistics but separated by sex... TenMileRace %&gt;% group_by(sex) %&gt;% summarise( xbar = mean(net), s = sd(net), cv = s / abs(xbar) ) ## # A tibble: 2 × 4 ## sex xbar s cv ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 F 5916.398 902.1090 0.1524761 ## 2 M 5280.702 929.9817 0.1761095 1.3.6 Empirical Rule of Thumb For any mound-shaped sample of data the following is a reasonable rule of thumb: Interval Approximate percent of Measurements \\(\\bar{x}\\pm s\\) 68% \\(\\bar{x}\\pm 2s\\) 95% \\(\\bar{x}\\pm 3s\\) 99.7% 1.4 Exercises O&amp;L 3.21. The ratio of DDE (related to DDT) to PCB concentrations in bird eggs has been shown to have had a number of biological implications. The ratio is used as an indication of the movement of contamination through the food chain. The paper “The ratio of DDE to PCB concentrations in Great Lakes herring gull eggs and its us in interpreting contaminants data” reports the following ratios for eggs collected at 13 study sites from the five Great Lakes. The eggs were collected from both terrestrial and aquatic feeding birds. Source Type DDE to PCB Ratio Terrestrial 76.50, 6.03, 3.51, 9.96, 4.24, 7.74, 9.54, 41.70, 1.84, 2.5, 1.54 Aquatic 0.27, 0.61, 0.54, 0.14, 0.63, 0.23, 0.56, 0.48, 0.16, 0.18 By hand, compute the mean and median separately for each type of feeder. Using your results from part (a), comment on the relative sensitivity of the mean and median to extreme values in a data set. Which measure, mean or median, would you recommend as the most appropriate measure of the DDE to PCB level for both types of feeders? Explain your answer. O&amp;L 3.31. Consumer Reports in its June 1998 issue reports on the typical daily room rate at six luxury and nine budget hotels. The room rates are given in the following table. Hotel Type Nightly Rate Luxury $175, $180, $120, $150, $120, $125 Budget $50, $50, $49, $45, $36, $45, $50, $50, $40 By hand, compute the means and standard deviations of the room rates for each class of hotel. Give a practical reason why luxury hotels might have higher variability than the budget hotels. (Don’t just say the standard deviation is higher because there is more spread in the data, but rather think about the Hotel Industry and why you might see greater price variability for upscale goods compared to budget items.) Use R to confirm your calculations in problem 1 (the pollution data). Show the code you used and the subsequent output. It will often be convenient for me to give you code that generates a data frame instead of uploading an Excel file and having you read it in. The data can be generated using the following commands: PolutionRatios &lt;- data.frame( Ratio = c(76.50, 6.03, 3.51, 9.96, 4.24, 7.74, 9.54, 41.70, 1.84, 2.5, 1.54, 0.27, 0.61, 0.54, 0.14, 0.63, 0.23, 0.56, 0.48, 0.16, 0.18 ), Type = c( rep(&#39;Terrestrial&#39;,11), rep(&#39;Aquatic&#39;,10) ) ) # Print out some of the data to confirm what the column names are head( PolutionRatios ) ## Ratio Type ## 1 76.50 Terrestrial ## 2 6.03 Terrestrial ## 3 3.51 Terrestrial ## 4 9.96 Terrestrial ## 5 4.24 Terrestrial ## 6 7.74 Terrestrial Hint: for computing the means and medians for each type of feeder separately, the group_by() command we demonstated earlier in the chapter is convenient. Use R to confirm your calculations in problem 2 (the hotel data). Show the code you used and the subsequent output. The data can be loaded into a data frame using the following commands Show the code you used and the subsequent output: Hotels &lt;- data.frame( Price = c(175, 180, 120, 150, 120, 125, 50, 50, 49, 45, 36, 45, 50, 50, 40), Type = c( rep(&#39;Luxury&#39;,6), rep(&#39;Budget&#39;, 9) ) ) # Print out some of the data to confirm what the column names are head( Hotels ) ## Price Type ## 1 175 Luxury ## 2 180 Luxury ## 3 120 Luxury ## 4 150 Luxury ## 5 120 Luxury ## 6 125 Luxury For the hotel data, create side-by-side box-and-whisker plots to compare the prices. Match the following histograms to the appropriate boxplot. Histogram A goes with boxplot __________ Histogram B goes with boxplot __________ Histogram C goes with boxplot __________ Histogram D goes with boxplot __________ Twenty-five employees of a corporation have a mean salary of $62,000 and the sample standard deviation of those salaries is $15,000. If each employee receives a bonus of $1,000, does the standard deviation of the salaries change? Explain your reasoning. "],
["2-probability.html", "Chapter 2 Probability 2.1 Introduction to Set Theory 2.2 Probability Rules 2.3 Discrete Random Variables 2.4 Common Discrete Distributions 2.5 Continuous Random Variables 2.6 Exercises", " Chapter 2 Probability # Every chapter, we will load all the librarys we will use at the beginning # of the chapter. library(ggplot2) # graphing functions library(dplyr) # data summary tools We need to work out the mathematics of what we mean by probability. To begin with we first define an outcome. An outcome is one observation from a random process or event. For example we might be interested in a single roll of a six-side die. Alternatively we might be interested in selecting one NAU student at random from the entire population of NAU students. 2.1 Introduction to Set Theory Before we jump into probability, it is useful to review a little bit of set theory. Events are properties of a particular outcome. For a coin flip, the event “Heads” would be the event that a heads was flipped. For the single roll of a six-sided die, a possible event might be that the result is even. For the NAU student, we might be interested in the event that the student is a biology student. A second event of interest might be if the student is an undergraduate. 1.1.1 Venn Diagrams Let \\(S\\) be the set of all outcomes of my random trial. Suppose I am interested in two events \\(A\\) and \\(B\\). The traditional way of representing these events is using a Venn diagram. For example, suppose that my random experiment is rolling a fair 6-sided die once. The possible outcomes are \\(S=\\{1,2,3,4,5,6\\}\\). Suppose I then define events \\(A=\\) roll is odd and \\(B=\\) roll is 5 or greater. In this case our picture is: All of our possible events are present, and distributed amongst our possible events. 2.1.1 Composition of events I am often interested in discussing the composition of two events and we give the common set operations below. Union: Denote the event that either \\(A\\) or \\(B\\) occurs as \\(A\\cup B\\). Denote the event that both \\(A\\) and \\(B\\) occur as \\(A\\cap B\\) Denote the event that \\(A\\) does not occur as \\(\\bar{A}\\) or \\(A^{C}\\) (different people use different notations) Definition 1. Two events \\(A\\) and \\(B\\) are said to be mutually exclusive (or disjoint) if the occurrence of one event precludes the occurrence of the other. For example, on a single roll of a die, a two and a five cannot both come up. For a second example, define \\(A\\) to be the event that the die is even, and \\(B\\) to be the event that the die comes up as a \\(5\\). 2.2 Probability Rules 2.2.1 Simple Rules We now take our Venn diagrams and use them to understand the rules of probability. The underlying idea that we will use is the the probability of an event is the area in the Venn diagram. Definition 2. Probability is the proportion of times an event occurs in many repeated trials of a random phenomenon. In other words, probability is the long-term relative frequency. Fact. For any event \\(A\\) the probability of the event \\(P(A)\\) satisfies \\(0\\le P(A) \\le 1\\) because proportions always lie in \\([0,1]\\). Because \\(S\\) is the set of all events that might occur, the area of our bounding rectangle will be \\(1\\) and the probability of event \\(A\\) occurring will be represented by the area in the circle \\(A\\). Fact. If two events are mutually exclusive, then \\(P(A\\cup B)=P(A)+P(B)\\) Example. Let \\(R\\) be the sum of two different colored dice. Suppose we are interested in \\(P(R \\le 4)\\). Notice that the pair of dice can fall 36 different ways (6 ways for the first die and six for the second results in 6x6 possible outcomes, and each way has equal probability \\(1/36\\). Because the dice cannot simultaneously sum to \\(2\\) and to \\(3\\), we could write \\[\\begin{aligned} P(R \\le 4 ) &amp;= P(R=2)+P(R=3)+P(R=4) \\\\ &amp;= P(\\left\\{ 1,1\\right\\} )+P(\\left\\{ 1,2\\right\\} \\mathrm{\\textrm{ or }}\\left\\{ 2,1\\right\\} )+P(\\{1,3\\}\\textrm{ or }\\{2,2\\}\\textrm{ or }\\{3,1\\}) \\\\ &amp;= \\frac{1}{36}+\\frac{2}{36}+\\frac{3}{36} \\\\ &amp;= \\frac{6}{36} \\\\ &amp;= \\frac{1}{6} \\end{aligned}\\] Fact. \\(P(A)+P(\\bar{A})=1\\) The above statement is true because the probability of whole space \\(S\\) is one (remember \\(S\\) is all possible outcomes), then either we get an outcome in which \\(A\\) occurs or we get an outcome in which \\(A\\) does not occur. Fact. \\(P(A\\cup B)=P(A)+P(B)-P(A\\cap B)\\) The reason behind this fact is that if there is if \\(A\\) and \\(B\\) are not disjoint, then some area is added twice when I calculate \\(P\\left(A\\right)+P\\left(B\\right)\\). To account for this, I simply subtract off the area that was double counted. Fact 3. \\(P(A)=P(A\\cap B)+P(A\\cap\\bar{B})\\) This identity is just breaking the event \\(A\\) into two disjoint pieces. 2.2.2 Conditional Probability We are given the following data about insurance claims. Notice that the data is given as \\(P(\\;Category\\;\\cap\\;PolicyType\\;)\\) which is apparent because the sum of all the elements in the table is \\(100\\%\\) \\(\\,\\) Fire Auto Other Fraudulant 6% 1% 3% non-Fraudulant 14% 29% 47% Summing across the rows and columns, we can find the probabilities of for each category and policy type. \\(\\,\\) Fire Auto Other \\(\\,\\) Fraudulant 6% 1% 3% 10% non-Fraudulant 14% 29% 47% 90% \\(\\,\\) 20% 30% 50% 100% It is clear that fire claims are more likely fraudulent than auto or other claims. In fact, the proportion of fraudulent claims, given that the claim is against a fire policy is \\[\\begin{aligned} P(\\textrm{ Fraud }|\\textrm{ FirePolicy }) &amp;= \\frac{\\textrm{proportion of claims that are fire policies and are fraudulent}}{\\textrm{proportion of fire claims}} \\\\ &amp;= \\frac{6\\%}{20\\%}\\\\ &amp; \\\\ &amp;= 0.3 \\end{aligned}\\] In general we define conditional probability (assuming \\(P(B) \\ne 0\\)) as \\[P(A|B)=\\frac{P(A\\cap B)}{P(B)}\\] which can also be rearranged to show \\[\\begin{aligned} P(A\\cap B) &amp;= P(A\\,|\\,B)\\,P(B) \\\\ &amp;= P(B\\,|\\,A)\\,P(A) \\end{aligned}\\] Because the order doesn’t matter and \\(P\\left(A\\cap B\\right)=P\\left(B\\cap A\\right)\\). Using this rule, we might calculate the probability that a claim is an Auto policy given that it is not fraudulent. \\[\\begin{aligned} P\\left(\\,Auto\\;|\\;NotFraud\\,\\right) &amp;= \\frac{P\\left(\\,Auto\\;\\cap\\;NotFraud\\right)}{P\\left(\\,NotFraud\\,\\right)} \\\\ &amp;= \\frac{0.29}{0.9} \\\\ &amp; \\\\ &amp;= 0.3\\bar{2} \\end{aligned}\\] Definition 4. Two events \\(A\\) and \\(B\\) are said to be independent if \\(P(A|B)=P(A)\\;\\;\\textrm{and}\\;\\;P(B|A)=P(B)\\). What independence is saying that knowing the outcome of event \\(A\\) doesn’t give you any information about the outcome of event \\(B\\). In simple random sampling, we assume that any two samples are independent. In cluster sampling, we assume that samples within a cluster are not independent, but clusters are independent of each other. Fact 5. If \\(A\\) and \\(B\\) are independent events, then \\(P(A\\cap B) = P(A|B)P(B) = P(A)P(B)\\). Example 6. Suppose that we are interested in the relationship between the color and the type of car. Specifically I will divide the car world into convertibles and non-convertibles and the colors into red and non-red. Suppose that convertibles make up just 10% of the domestic automobile market. This is to say \\(P(\\;Convertable\\;)=0.10\\). Of the non-convertibles, red is not unheard of but it isn’t common either. So suppose \\(P(\\;Red\\;|\\;NonConvertable\\;)=0.15\\). However red is an extremely popular color for convertibles so let \\(P(\\;Red\\;|\\;Convertable\\;)=0.60\\). Given the above information, we can create the following table: \\(\\,\\) Convertable Not Convertable \\(\\,\\) Red Not Red \\(\\,\\) 10% 90% 100% We can fill in some of the table using our the definition of conditional probability. For example: \\[\\begin{aligned} P\\left(Red\\,\\cap\\,Convertable\\right) &amp;= P\\left(Red\\,|\\,Convertable\\right)\\,P\\left(Convertable\\right) \\\\ &amp;= 0.60*0.10 \\\\ &amp;= 0.06 \\end{aligned}\\] Lets think about what this conditional probability means. Of the \\(90\\%\\) of cars that are not convertibles, \\(15\\%\\) those non-convertibles are red and therefore the proportion of cars that are red non-convertibles is \\(0.90*0.15=0.135\\). Of the \\(10\\%\\) of cars that are convertibles, \\(60\\%\\) of those are red and therefore proportion of cars that are red convertibles is \\(0.10*0.60=0.06\\). Thus the total percentage of red cars is actually \\[\\begin{aligned}P\\left(\\,Red\\,\\right) &amp;= P\\left(\\;Red\\;\\cap\\;Convertible\\;\\right)+P\\left(\\,Red\\,\\cap\\,NonConvertible\\,\\right)\\\\ &amp;= P\\left(\\,Red\\,|\\,Convertable\\,\\right)P\\left(\\,Convertible\\,\\right)+P\\left(\\,Red\\,|\\,NonConvertible\\,\\right)P\\left(\\,NonConvertible\\,\\right)\\\\ &amp;= 0.60*0.10+0.15*0.90\\\\ &amp;= 0.06+0.135\\\\ &amp;= 0.195 \\end{aligned}\\] So when I ask for \\(P(\\;red\\;|\\;convertable\\;)\\), I am narrowing my space of cars to consider only convertibles. While there percentage of cars that are red and convertible is just 6% of all cars, when I restrict myself to convertibles, we see that the percentage of this smaller set of cars that are red is 60%. Notice that because \\(P\\left(Red\\right)=0.195\\ne0.60=P\\left(Red\\,|\\,Convertable\\right)\\) then the events \\(Red\\) and \\(Convertable\\) are not independent. 2.2.3 Summary of Probability Rules \\[0 \\le P\\left(A\\right) \\le 1\\] \\[P\\left(A\\right)+P\\left(\\bar{A}\\right)=1\\] \\[P\\left(A\\cup B\\right) = P\\left(A\\right)+P\\left(B\\right)-P\\left(A\\cap B\\right)\\] \\[P\\left(A\\cap B\\right) = \\begin{cases} P\\left(A\\,|\\,B\\right)P\\left(B\\right)\\\\ P\\left(B\\,|\\,A\\right)P\\left(A\\right)\\\\ P(A)P(B)\\;\\; &amp; \\textrm{ if A,B are independent} \\end{cases}\\] \\[P\\left(A\\,|\\,B\\right) = \\frac{P\\left(A\\cap B\\right)}{P\\left(B\\right)}\\] 2.3 Discrete Random Variables The different types of probability distributions (and therefore your analysis method) can be divided into two general classes: Continuous Random Variables - the variable takes on numerical values and could, in principle, take any of an uncountable number of values. In practical terms, if fractions or decimal points in the number make sense, it is usually continuous. Discrete Random Variables - the variable takes on one of small set of values (or only a countable number of outcomes). In practical terms, if fractions or decimals points don’t make sense, it is usually discrete. Examples: Presence or Absence of wolves in a State? Number of Speeding Tickets received? Tree girth (in cm)? Photosynthesis rate? 2.3.1 Introduction to Discrete Random Variables The following facts hold for discrete random variables: The probability associated with every value lies between 0 and 1 The sum of all probabilities for all values is equal to 1 Probabilities for discrete RVs are additive. i.e., \\(P(3\\textrm{ or }4)=P(3)+P(4)\\) 2.3.1.1 Expected Value Example: Consider the discrete random variable \\(S\\), the sum of two fair dice. We often want to ask ‘What is expected value of this distribution?’ You might think about taking a really, really large number of samples from this distribution and then taking the mean of that really really big sample. We define the expected value (often denoted by \\(\\mu\\)) as a weighted average of the possible values and the weights are the proportions with which those values occur. \\[\\mu=E[S] = \\sum_{\\textrm{possible }s}\\;s\\cdot P\\left(S=s\\right)\\] In this case, we have that \\[\\begin{aligned} \\mu = E[S] &amp;= \\sum_{s=2}^{12}s\\cdot P(S=s) \\\\ &amp;= 2\\cdot P\\left(S=2\\right)+3\\cdot P\\left(S=3\\right)+\\dots+11\\cdot P\\left(S=11\\right)+12\\cdot P\\left(S=12\\right) \\\\ &amp;= 2\\left(\\frac{1}{36}\\right)+3\\left(\\frac{2}{36}\\right)+\\dots+11\\left(\\frac{2}{36}\\right)+12\\left(\\frac{1}{36}\\right) \\\\ &amp;= 7 \\end{aligned}\\] 2.3.1.2 Variance Similarly we could define the variance of \\(S\\) (which we often denote \\(\\sigma^{2}\\)) as a weighted average of the squared-deviations that could occur. \\[ \\sigma^{2}=V[S] = \\sum_{\\textrm{possible }s}\\; (s-\\mu)^2 \\cdot P\\left(S=s\\right)\\] which in this example can be calculated as \\[\\begin{aligned} \\sigma^{2}=V[S] &amp;= \\sum_{s=2}^{12}\\left(s-\\mu\\right)^{2}P(S=s) \\\\ &amp;= (2-7)^{2}\\left(\\frac{1}{36}\\right)+(3-7)^{2}\\left(\\frac{2}{36}\\right)+\\dots+(12-7)^{2}\\left(\\frac{1}{36}\\right) \\\\ &amp;= \\frac{35}{6}=5.8\\bar{3} \\end{aligned}\\] We could interpret the expectation as the sample mean of an infinitely large sample, and the variance as the sample variance of the same infinitely large sample. These are two very important numbers that describe the distribution. Example 7. My wife is a massage therapist and over the last year, the number of clients she sees per work day (denoted Y) varied according the following table: Number of Clients 0 1 2 3 4 Frequency/Probability 0.30 0.35 0.20 0.10 0.05 distr &lt;- data.frame( clients = c( 0, 1, 2, 3, 4 ), # two columns probability = c(0.3, 0.35, 0.20, 0.10, 0.05 ) ) # ggplot(distr, aes(x=clients)) + # graph with clients as the x-axis geom_point(aes(y=probability)) + # where the dots go geom_linerange(aes(ymax=probability, ymin=0)) + # the vertical lines theme_bw() # set background color... Because this is the long term relative frequency of the number of clients (over 200 working days!), it is appropriate to interpret these frequencies as probabilities. This table and graph is often called a probability mass function (pmf) because it lists how the probability is spread across the possible values of the random variable. We might next ask ourselves what is the average number of clients per day? It looks like it ought to be between 1 and 2 clients per day. \\[\\begin{aligned} E\\left(Y\\right) &amp;= \\sum_{\\textrm{possible }y}y\\,P\\left(Y=y\\right) \\\\ &amp;= \\sum_{y=0}^{4}y\\,P\\left(Y=y\\right) \\\\ &amp;= 0\\,P\\left(Y=0\\right)+1\\,P\\left(Y=1\\right)+2\\,P\\left(Y=2\\right)+3\\,P\\left(Y=3\\right)+4\\,P\\left(Y=4\\right) \\\\ &amp;= 0\\left(0.3\\right)+1\\left(0.35\\right)+2\\left(0.20\\right)+3\\left(0.10\\right)+4\\left(0.05\\right) \\\\ &amp;= 1.25 \\end{aligned}\\] Assuming that successive days are independent (which might be a bad assumption) what is the probability she has two days in a row with no clients? \\[\\begin{aligned}P\\left(\\textrm{0 on day1 }and\\textrm{ 0 on day2}\\right) &amp;= P\\left(\\textrm{0 on day 1}\\right)P\\left(\\textrm{0 on day 2}\\right) \\\\ &amp;= \\left(0.3\\right)\\left(0.3\\right) \\\\ &amp;= 0.09 \\end{aligned}\\] What is the variance of this distribution? \\[\\begin{aligned}V\\left(Y\\right) &amp;= \\sum_{\\textrm{possible y}}\\,\\left(y-\\mu\\right)^{2}\\,P\\left(Y=y\\right) \\\\ &amp;= \\sum_{y=0}^{4}\\,\\left(y-\\mu\\right)^{2}P\\left(Y=y\\right) \\\\ &amp;= \\left(0-1.25\\right)^{2}\\left(0.3\\right)+\\left(1-1.25\\right)^{2}\\left(0.35\\right)+\\left(2-1.25\\right)^{2}\\left(0.20\\right)+\\left(3-1.25\\right)^{2}\\left(0.10\\right)+\\left(4-1.25\\right)^{2}\\left(0.05\\right) \\\\ &amp;= 1.2875 \\end{aligned}\\] Note on Notation: There is a difference between the upper and lower case letters we have been using to denote a random variable. In general, we let the upper case denote the random variable and the lower case as a value that the the variable could possibly take on. So in the massage example, the number of clients seen per day \\(Y\\) could take on values \\(y=0,1,2,3,\\) or \\(4\\). 2.4 Common Discrete Distributions 2.4.1 Binomial Distribution Example: Suppose we are trapping small mammals in the desert and we spread out three traps. Assume that the traps are far enough apart that having one being filled doesn’t affect the probability of the others being filled and that all three traps have the same probability of being filled in an evening. Denote the event that a trap is filled with a critter as \\(C_{i}\\) and denote the event that the trap is empty as \\(E_{i}\\). Denote the probability that a trap is filled by \\(\\pi=0.8\\). (This sort of random variable is often referred to as a Bernoulli RV.) The possible outcomes are Outcome \\(\\,\\) \\(E_1, E_2, E_3\\) \\(\\,\\) \\(C_1, E_2, E_3\\) \\(\\,\\) \\(E_1, C_2, E_3\\) \\(\\,\\) \\(E_1, E_2, C_3\\) \\(\\,\\) \\(C_1, C_2, E_3\\) \\(\\,\\) \\(C_1, E_2, C_3\\) \\(\\,\\) \\(E_1, C_2, C_3\\) \\(\\,\\) \\(C_1, C_2, C_3\\) \\(\\,\\) Because these are far apart enough in space that the outcome of Trap1 is independent of Trap2 and Trap3, then \\[P(E_{1}\\cap C_{2}\\cap E_{3}) = P(E_{1})P(C_{2})P(E_{3}) = (1-0.8)0.8(1-0.8) = 0.032\\] Notice how important the assumption of independence is!!! Similarly we could calculate the probabilities for the rest of the table. Outcome Probability \\(S\\) Outcome Probability \\(E_1, E_2, E_3\\) 0.008 \\(S=0\\) 0.008 ——————- ————— ————- ————— \\(C_1, E_2, E_3\\) 0.032 \\(E_1, C_2, E_3\\) 0.032 \\(S=1\\) \\(3(0.032) = 0.096\\) \\(E_1, E_2, C_3\\) 0.032 ——————- ————— ————- ————— \\(C_1, C_2, E_3\\) 0.128 \\(C_1, E_2, C_3\\) 0.128 \\(S=2\\) \\(3(0.128) = 0.384\\) \\(E_1, C_2, C_3\\) 0.128 ——————- ————— ————- ————— \\(C_1, C_2, C_3\\) 0.512 \\(S=3\\) \\(0.512\\) Next we are interested in the random variable \\(S\\), the number of traps that were filled: \\(S\\) Outcome Probability \\(S=0\\) \\(0.008\\) \\(S=1\\) \\(0.096\\) \\(S=2\\) \\(0.384\\) \\(S=3\\) \\(0.512\\) \\(S\\) is an example of a Binomial Random Variable. A binomial experiment is one that: Experiment consists of \\(n\\) identical trials. Each trial results in one of two outcomes (Heads/Tails, presence/absence). One will be labeled a success and the other a failure. The probability of success on a single trial is equal to \\(\\pi\\) and remains the same from trial to trial. The trials are independent (this is implied from property 3). The random variable \\(Y\\) is the number of successes observed during \\(n\\) trials. Recall that the probability mass function (pmf) describes how the probability is spread across the possible outcomes, and in this case, I can describe this via a nice formula. The pmf of a a binomial random variable \\(X\\) taken from \\(n\\) trials each with probability of success \\(\\pi\\) is \\[P(X=x)=\\underbrace{\\frac{n!}{x!(n-x)!}}_{orderings}\\;\\underbrace{\\pi^{x}}_{y\\,successes}\\;\\underbrace{(1-\\pi)^{n-x}}_{n-y\\,failures}\\] where we define \\(n!=n(n-1)\\dots(2)(1)\\) and further define \\(0!=1\\). Often the ordering term is written more compactly as \\[{n \\choose x}=\\frac{n!}{x!\\left(n-x\\right)!}\\]. For our small mammal example we can create a graph that shows the binomial distribution with the following R code: dist &lt;- data.frame( x=0:3 ) %&gt;% mutate(probability = dbinom(x, size=3, prob=0.8)) ggplot(dist, aes(x=x)) + geom_point(aes(y=probability)) + geom_linerange(aes(ymax=probability, ymin=0)) + ggtitle(&#39;Binomial distribution: n=3, p=0.8&#39;) + theme_bw() To calculate the height of any of these bars, we can evaluate the pmf at the desired point. For example, to calculate the probability the number of full traps is 2, we calculate the following \\[\\begin{aligned} P(X=2) &amp;= {3 \\choose 2}\\left(0.8\\right)^{2}\\left(1-0.8\\right)^{3-2} \\\\ &amp;= \\frac{3!}{2!(3-2)!}(0.8)^{2}(0.2)^{3-2} \\\\ &amp;= \\frac{3\\cdot2\\cdot1}{(2\\cdot1)1}\\;(0.8)^{2}(0.2) \\\\ &amp;= 3(0.128) \\\\ &amp;= 0.384 \\end{aligned}\\] You can use R to calculate these probabilities. In general, for any distribution, the “d-function” gives the distribution function (pmf or pdf). So to get R to do the preceding calculation we use: # If X ~ Binomial(n=3, pi=0.8) # Then P( X = 2 | n=3, pi=0.8 ) = dbinom(2, size=3, prob=0.8) ## [1] 0.384 The expectation of this distribution can be shown to be \\[\\begin{aligned}E[X] &amp;= \\sum_{x=0}^{n}x\\,P(X=x) \\\\ &amp;= \\sum_{x=0}^{n}x\\;\\frac{n!}{x!\\left(n-x\\right)!}\\pi^{x}\\left(1-\\pi\\right)^{n-x}\\\\ &amp;= \\vdots \\\\ &amp;= n\\pi \\end{aligned}\\] and the variance can be similarly calculated \\[\\begin{aligned} V[X] &amp;= \\sum_{x=0}^{n}\\left(x-E\\left[X\\right]\\right)^{2}\\,P\\left(X=x|n,\\pi\\right) \\\\ &amp;= \\sum_{x=0}^{n}\\left(x-E\\left[X\\right]\\right)^{2}\\;\\frac{n!}{x!\\left(n-x\\right)!}\\pi^{x}\\left(1-\\pi\\right)^{n-x} \\\\ &amp;= \\vdots \\\\ &amp;= n\\pi(1-\\pi) \\end{aligned}\\] Example 8. Suppose a bird survey only captures the presence or absence of a particular bird (say the mountain chickadee). Assuming the true presence proportion at national forest sites around Flagstaff \\[\\pi=0.1\\], then for \\(n=20\\) randomly chosen sites, the number of sites in which the bird was observed would have the distribution dist &lt;- data.frame( x = 0:20 ) %&gt;% mutate(probability = dbinom(x, size=20, prob=0.1)) ggplot(dist, aes(x=x)) + geom_point(aes(y=probability)) + geom_linerange(aes(ymax=probability, ymin=0)) + ggtitle(&#39;Binomial distribution: n=20, p=0.1&#39;) + xlab(&#39;Number of Sites Occupied&#39;) + theme_bw() Often we are interested in questions such as \\(P(X\\le2)\\) which is the probability that we see 2 or fewer of the sites being occupied by mountain chickadee. These calculations can be tedious to calculate by hand but R will calculate these cumulative distribution function values for you using the “p-function”. This cumulative distribution function gives the sum of all values up to and including the number given. # P(X=0) + P(X=1) + P(X=2) sum &lt;- dbinom(0, size=20, prob=0.1) + dbinom(1, size=20, prob=0.1) + dbinom(2, size=20, prob=0.1) sum ## [1] 0.6769268 # P(X &lt;= 2) pbinom(2, size=20, prob=0.1) ## [1] 0.6769268 In general we will be interested in asking four different questions about a distribution. What is the height of the probability mass function (or probability density function). For discrete variable \\(Y\\) this is \\(P\\left(Y=y\\right)\\) for whatever value of \\(y\\) we want. In R, this will be the d-function. What is the probability of observing a value less than or equal to \\(y\\)? In other words, to calculate \\(P\\left(Y\\le y\\right)\\). In R, this will be the p-function. What is a particular quantile of a distribution? For example, what value separates the lower \\(25\\%\\) from the upper \\(75\\%\\)? In R, this will be the q-function. Generate a random sample of values from a specified distribution. In R, this will be the r-function. 2.4.2 Poisson Distribution A commonly used distribution for count data is the Poisson. Number of customers arriving over a 5 minute interval Number of birds observed during a 10 minute listening period Number of prairie dog towns per 1000 hectares Number of alga clumps per cubic meter of lake water For a RV is a Poisson RV if the following conditions apply: Two or more events do not occur at precisely the same time or in the same space The occurrence of an event in a given period of time or region of space is independent of the occurrence of the event in a non overlapping period or region. The expected number of events during one period or region, \\(\\lambda\\), is the same in all periods or regions of the same size. Assuming that these conditions hold for some count variable \\(Y\\), the the probability mass function is given by \\[P(Y=y)=\\frac{\\lambda^{y}e^{-\\lambda}}{y!}\\] where \\(\\lambda\\) is the expected number of events over 1 unit of time or space and \\(e\\) is the constant \\(2.718281828\\dots\\). \\[E[Y] = \\lambda\\] \\[Var[Y] = \\lambda\\] Example 9. Suppose we are interested in the population size of small mammals in a region. Let \\(Y\\) be the number of small mammals caught in a large trap (multiple traps in the same location?) in a 12 hour period. Finally, suppose that \\(Y\\sim Poi(\\lambda=2.3)\\). What is the probability of finding exactly 4 critters in our trap? \\[P(Y=4) = \\frac{2.3^{4}\\,e^{-2.3}}{4!} = 0.1169\\] What about the probability of finding at most 4? \\[\\begin{aligned} P(Y\\le4) &amp;= P(Y=0)+P(Y=1)+P(Y=2)+P(Y=3)+P(Y=4) \\\\ &amp;= 0.1003+0.2306+0.2652+0.2033+0.1169 \\\\ &amp;= 0.9163 \\end{aligned}\\] What about the probability of finding 5 or more? \\[P(Y\\ge5) = 1-P(Y\\le4) = 1-0.9163 = 0.0837\\] These calculations can be done using the distribution function (d-function) for the poisson and the cumulative distribution function (p-function). dist &lt;- data.frame( NumCaught = 0:10 ) %&gt;% mutate( probability = dpois( NumCaught, lambda=2.3 ) ) ggplot(dist, aes(x=NumCaught)) + geom_point( aes(y=probability) ) + geom_linerange(aes( ymax=probability, ymin=0)) + ggtitle(expression(paste(&#39;Poisson Distribution with &#39;, lambda == 2.3))) + labs(x=&#39;Number Caught&#39;) + theme_bw() # P( Y = 4) dpois(4, lambda=2.3) ## [1] 0.1169022 # P( Y &lt;= 4) ppois(4, lambda=2.3) ## [1] 0.9162493 # 1-P(Y &lt;= 4) == P( Y &gt; 4) == P( Y &gt;= 5) 1-ppois(4, 2.3) ## [1] 0.08375072 2.5 Continuous Random Variables Continuous random variables can take on an (uncountably) infinite number of values, and this results in a few obnoxious mathematical differences between how we handle continuous and discrete random variables. In particular, the probability that a continuous random variable \\(X\\) will take on a particular value will be zero, so we will be interested in finding the probability that the random variable is in some interval instead. Wherever we had a summation, \\(\\sum\\), we will instead have an integral, but because many students haven’t had calculus, we will resort to using R or tables of calculated values. 2.5.1 Uniform(0,1) Distribution Suppose you wish to draw a random number number between 0 and 1 and any two intervals of equal size should have the same probability of the value being in them. This random variable is said to have a Uniform(0,1) distribution. Because there are an infinite number of rational numbers between 0 and 1, the probability of any particular number being selected is \\(1/\\infty=0\\). But even though each number has 0 probability of being selected, some number must end up being selected. Because of this conundrum, probability theory doesn’t look at the probability of a single number, but rather focuses on a region of numbers. To make this distinction, we will define the distribution using a probability density function (pdf) instead of the probability mass function. In the discrete case, we had to constrain the probability mass function to sum to 1. In the continuous case, we have to constrain the probability density function to integrate to 1. Finding the area under the curve of a particular density function \\(f(x)\\) usually requires the use of calculus, but since this isn’t a calculus course, we will resort to using R or tables of calculated values. 2.5.2 Exponential Distribution The exponential distribution is the continuous analog of the Poisson distribution and is often used to model the time between occurrence of successive events. Perhaps we are modeling time between transmissions on a network, or the time between feeding events or prey capture. If the random variable \\(X\\) has an Exponential distribution, its probability density function is \\[f(x)=\\begin{cases} \\lambda e^{-\\lambda x} &amp; x\\ge0\\;\\textrm{ and }\\;\\lambda&gt;0\\\\ 0 &amp; \\textrm{otherwise} \\end{cases}\\] Analogous to the discrete distributions, we can define the Expectation and Variance of these distributions by replacing the summation with an integral \\[\\mu = E[X] = \\int_{0}^{\\infty}x\\,f(x)\\,dx = \\dots = \\frac{1}{\\lambda} \\] \\[\\sigma^2 = Var[X] = \\int_{0}^{\\infty}\\left(x-\\mu\\right)^{2}\\,f\\left(x\\right)\\,dx = \\dots = \\frac{1}{\\lambda^{2}}\\] Because the exponential distribution is defined by the rate of occurrence of an event, increasing that rate decreases the time between events. Furthermore because the rate of occurrence cannot be negative, we restrict \\(\\lambda&gt;0\\). Example 10. Suppose the time between insect captures \\(X\\) during a summer evening for a species of bat follows a exponential distribution with capture rate of \\(\\lambda=2\\) insects per minute and therefore the expected waiting time between captures is \\(1/\\lambda=1/2\\) minute. Suppose that we are interested in the probability that it takes a bat more than 1 minute to capture its next insect. \\[P(X&gt;1)=\\] data &lt;- data.frame(x=seq(0,5,length=1000), lambda = 2) %&gt;% mutate(y=dexp(x, rate = lambda), grp = ifelse( x &gt; 1, &#39;&gt; 1&#39;, &#39;&lt;= 1&#39;)) ggplot(data, aes(x=x, y=y, fill=grp)) + geom_area() + labs(y=&#39;density&#39;) + theme_bw() We now must resort to calculus to find this area. Or use tables of pre-calculated values. Or use R, remembering that p-functions give the area under the curve to the left of the given value. # P(X &gt; 1) == 1 - P(X &lt;= 1) 1 - pexp(1, rate=2) ## [1] 0.1353353 2.5.3 Normal Distribution Undoubtably the most important distribution in statistics is the normal distribution. If my RV \\(X\\) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), its probability density function is given by \\[f(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[\\frac{-(x-\\mu)^{2}}{2\\sigma^{2}}\\right]\\] where \\(\\exp[y]\\) is the exponential function \\(e^{y}\\). We could slightly rearrange the function to \\[f(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right]\\] and see this distribution is defined by its expectation \\(E[X]=\\mu\\) and its variance \\(Var[X]=\\sigma^{2}\\). Notice I could define it using the standard deviation \\(\\sigma\\), and different software packages will expect it to be defined by one or the other. R defines the normal distribution using the standard deviation. Example 11. It is known that the heights of adult males in the US is approximately normal with a mean of 5 feet 10 inches (\\(\\mu=70\\) inches) and a standard deviation of \\(\\sigma=3\\) inches. Your instructor is a mere 5 feet 4 inches (64 inches). What proportion of the population is shorter than your professor? distr &lt;- data.frame(x=seq(57, 82, length=1000)) %&gt;% mutate( density = dnorm(x, mean=70, sd=3), group = ifelse(x&lt;=64, &#39;Shorter&#39;,&#39;Taller&#39;) ) ggplot(distr, aes(x=x, y=density, fill=group)) + geom_line() + geom_area() + theme_bw() Using R you can easily find this pnorm(64, mean=70, sd=3) ## [1] 0.02275013 2.5.4 Standardizing Before we had computers that could calculate these probabilities for any normal distribution, it was important to know how to convert a probability statement from an arbitrary \\(N\\left(\\mu,\\sigma^{2}\\right)\\) distribution to a question about a Stanard Normal distribution, which is a normal distribution with mean \\(\\mu=0\\) and standard deviation \\(\\sigma=1\\). If we have \\[X\\sim N\\left(\\mu,\\sigma^{2}\\right)\\] then \\[Z=\\frac{X-\\mu}{\\sigma}\\sim N\\left(0,1\\right)\\] You might remember doing something similar in an undergraduate statistics course in order to use a table to look up some probability. From the height example, we calculate \\[\\begin{aligned}z &amp;= \\frac{64-70}{3} \\\\ &amp;= \\frac{-6}{3} \\\\ &amp;= -2 \\end{aligned}\\] Note that this calculation shows that he is \\(-2\\) standard deviations from the mean. Next we look at a table for \\(z=-2.00\\). To do this we go down to the \\(-2.0\\) row and over to the \\(.00\\) column and find \\(0.0228\\). Only slightly over 2% of the adult male population is shorter! How tall must a person be to be taller than \\(80\\%\\) of the rest of the adult male population? To answer that we must use the table in reverse and look for the \\(0.8\\) value. We find the closest value possible \\((0.7995)\\) and the \\(z\\) value associated with it is \\(z=0.84\\). Next we solve the standardizing equation for \\(x\\) \\[\\begin{aligned} z &amp;= \\frac{x-\\mu}{\\sigma} \\\\ 0.84 &amp;= \\frac{x-70}{3} \\\\ x &amp;= 3(0.84)+70 \\\\ &amp;= 72.49\\;\\textrm{inches} \\end{aligned}\\] Alternatively we could use the quantile function for the normal distribution (q-function) in R and avoid the imprecision of using a table. qnorm(.8, mean=0, sd=1) ## [1] 0.8416212 Empirical Rule - It is from the normal distribution that the empirical rule from the previous chapter is derived. If \\(X\\sim N(\\mu,\\sigma^{2})\\) then \\[\\begin{aligned} P(\\mu-\\sigma\\le X\\le\\mu+\\sigma) &amp;= P(-1 \\le Z \\le 1) \\\\ &amp;= P(Z \\le 1) - P(Z \\le -1) \\\\ &amp;\\approx 0.8413-0.1587 \\\\ &amp;= 0.6826 \\end{aligned}\\] 1.6 R Comments There will be a variety of distributions we’ll be interested in and R refers to them using the following abbreviations Distribution Stem Parameters Parameter Interpretation Binomial binom size prob Number of Trials, Probability of Success (per Trial) Exponential exp rate Mean of the distribution Normal norm mean=0 sd=1 Center of the distribution, Standard deviation Uniform unif min=0 max=1 Minimum and Maximum of the distribution All the probability distributions available in R are accessed in exactly the same way, using a d-function, p-function, q-function, and r-function. Function Result d-function(x) The height of the probability distribution/density at \\(x\\) p-function(x) \\(P\\left(X\\le x\\right)\\) q-function(q) \\(x\\) such that \\(P\\left(X\\le x\\right) = q\\) r-function(n) \\(n\\) random observations from the distribution 2.6 Exercises The population distribution of blood donors in the United States based on race/ethnicity and blood type as reported by the American Red Cross is given here: \\(\\,\\) O A B AB Total White 36% 32.2% 8.8% 3.2% \\(\\,\\) Black 7% 2.9% 2.5% 0.5% \\(\\,\\) Asian 1.7% 1.2% 1% 0.3% \\(\\,\\) Other 1.5% 0.8% 0.3% 0.1% \\(\\,\\) \\(\\,\\) \\(\\,\\) \\(\\,\\) \\(\\,\\) \\(\\,\\) 100% Notice that the numbers given in the table sum to 100%, so the data presented are the probability of a particular ethnicity and blood type. Fill in the column and row totals. What is the probability that a randomly selected donor will be Asian and have Type O blood? That is to say, given a donor is randomly selected from the list of all donors, what is the probability that the selected donor will Asian with Type O? What is the probability that a randomly selected donor is white? That is to say, given a donor is randomly selected from the list of all donors, what is the probability that the selected donor is white? What is the probability that a randomly selected donor has Type A blood? That is to say, given a donor is selected from the list of all donors, what is the probability that the selected donor has Type A blood? What is the probability that a white donor will have Type A blood? That is to say, given a donor is randomly selected from the list of all the white donors, what is the probability that the selected donor has Type A blood? (Notice we already know the donor is white because we restricted ourselves to that subset!) Is blood type and ethnicity independent? Justify your response mathematically using your responses from the previous answers. For each of the following, mark if it is Continuous or Discrete. \\(\\underline{\\hspace{1in}}\\) Milliliters of tea drunk per day. \\(\\underline{\\hspace{1in}}\\) Different brands of soda drunk over the course of a year. \\(\\underline{\\hspace{1in}}\\) Number of days per week that you are on-campus for any amount of time. \\(\\underline{\\hspace{1in}}\\) Number of grizzly bears individuals genetically identified from a grid of hair traps in Glacier National Park. For each scenario, state whether the event should be modeled via a binomial or Poisson distribution. \\(\\underline{\\hspace{1in}}\\) Number of M&amp;Ms I eat per hour while grading homework \\(\\underline{\\hspace{1in}}\\) The number of mornings in the coming 7 days that I change my son’s first diaper of the day. \\(\\underline{\\hspace{1in}}\\) The number of Manzanita bushes per 100 meters of trail. During a road bike race, there is always a chance a crash will occur. Suppose the probability that at least one crash will occur in any race I’m in is \\(\\pi=0.2\\) and that races are independent. What is the probability that the next two races I’m in will both have crashes? What is the probability that neither of my next two races will have a crash? What is the probability that at least one of the next two races have a crash? My cats suffer from gastric distress due to eating house plants and the number of vomits per week that I have to clean up follows a Poisson distribution with rate \\(\\lambda=1.2\\) pukes per week. What is the probability that I don’t have to clean up any vomits this coming week? What is the probability that I must clean up 1 or more vomits? If I wanted to measure this process with a rate per day, what rate should I use? Suppose that the number of runners I see on a morning walk on the trails near my house has the following distribution (Notice I’ve never seen four or more runners on a morning walk): y 0 1 2 3 4+ Probabilty 0.45 0.25 0.20 0.0 a) What is the probability that I see 3 runners on a morning walk? b) What is the expected number of runners that I will encounter? c) What is the variance of the number of runners that I will encounter? If \\(Z\\sim N\\left(\\mu=0,\\sigma^{2}=1\\right)\\), find the following probabilities: \\(P(Z&lt;1.58)=\\) \\(P(Z=1.58)=\\) \\(P(Z&gt;-.27)=\\) \\(P(-1.97&lt;Z&lt;2.46)=\\) Using the Standard Normal Table or the table functions in R, find \\(z\\) that makes the following statements true. \\(P(Z&lt;z)=.75\\) \\(P(Z&gt;z)=.4\\) The amount of dry kibble that I feed my cats each morning can be well approximated by a normal distribution with mean \\(\\mu=200\\) grams and standard deviation \\(\\sigma=30\\) grams. What is the probability that I fed my cats more than 250 grams of kibble this morning? From my cats’ perspective, more food is better. How much would I have to feed them for this morning to be among the top \\(10\\%\\) of feedings? "],
["3-confidence-intervals-via-bootstrapping.html", "Chapter 3 Confidence Intervals via Bootstrapping 3.1 Theory of Bootstrapping 3.2 Quantile-based Confidence Intervals 3.3 Exercises", " Chapter 3 Confidence Intervals via Bootstrapping # Every chapter, we will load all the librarys we will use at the beginning # of the chapter. library(mosaic) # for the resample, shuffle, and do functions library(ggplot2) # graphing functions library(dplyr) # data summary tools 3.1 Theory of Bootstrapping Suppose that we had a population of interest and we wish to estimate the mean of that population (the population mean we’ll denote as \\(\\mu\\)). We can’t observe every member of the population (which would be prohibitively expensive) so instead we take a random sample and from that sample calculate a sample mean (which we’ll denote \\(\\bar{x}\\)). We believe that \\(\\bar{x}\\) will be a good estimator of \\(\\mu\\), but it will vary from sample to sample and won’t be exactly equal to \\(\\mu\\). Next suppose we wish to ask if a particular value for \\(\\mu\\), say \\(\\mu_{0}\\), is consistent with our observed data? We know that \\(\\bar{x}\\) will vary from sample to sample, but we have no idea how much it will vary between samples. However, if we could understand how much \\(\\bar{x}\\) varied sample to sample, we could answer the question. For example, suppose that \\(\\bar{x}=5\\) and we know that \\(\\bar{x}\\) varied about \\(\\pm2\\) from sample to sample. Then I’d say that possible values of \\(\\mu_{0}\\) in the interval \\(3\\) to \\(7\\) \\(\\left(5\\pm2\\right)\\) are reasonable values for \\(\\mu\\) and anything outside that interval is not reasonable. Therefore, if we could take many, many repeated samples from the population and calculate our test statistic \\(\\bar{x}\\) for each sample, we could rule out possible values of \\(\\mu\\). Unfortunately we don’t have the time or money to repeatedly sample from the actual population, but we could sample from our best approximation to what the population is like. Suppose we were to sample from a population of shapes, and we observed \\(4/9\\) of the sample were squares, \\(3/9\\) were circles, and a triangle and a diamond. Then our best guess of what the population that we sampled from was a population with \\(4/9\\) squares, \\(3/9\\) circles, and \\(1/9\\) of triangles and diamonds. multiplot(sample.plot, population.plot, cols=2) ## Loading required package: grid Using this approximated population (which is just many many copies of our sample data), we can repeated sample \\(\\bar{x}^{*}\\) values to create an estimate of the sampling distribution of \\(\\bar{x}\\). Because our approximate population is just an infinite number of copies of our sample data, then sampling from the approximate population is equivalent to sampling with replacement from our sample data. If I take \\(n\\) samples from \\(n\\) distinct objects with replacement, then the process can be thought of as mixing the \\(n\\) objects in a bowl and taking an object at random, noting which it is, replace it into the bowl, and then draw the next sample. Practically, this means some objects will be selected more than once and some will not be chosen at all. To sample our observed data with replacement, we’ll use the resample() function in the mosaic package. We see that some rows will be selected multiple times, and some will not be selected at all. Testing.Data &lt;- data.frame( name=c(&#39;Alison&#39;,&#39;Brandon&#39;,&#39;Chelsea&#39;,&#39;Derek&#39;,&#39;Elise&#39;)) Testing.Data ## name ## 1 Alison ## 2 Brandon ## 3 Chelsea ## 4 Derek ## 5 Elise # Sample rows from the Testing Data (with replacement) resample(Testing.Data) ## name orig.id ## 1 Alison 1 ## 4 Derek 4 ## 3 Chelsea 3 ## 1.1 Alison 1 ## 5 Elise 5 Notice Alison has selected twice, while Brandon has not been selected at all. The sampling from the estimated population via sampling from the observed data is called bootstrapping because we are making no distributional assumptions about where the data came from, and the idiom “Pulling yourself up by your bootstraps” seemed appropriate. Example: Mercury Levels in Fish from Florida Lakes A data set provided by the Lock\\(^{5}\\) introductory statistics textbook looks at the mercury levels in fish harvested from lakes in Florida. There are approximately 7,700 lakes in Florida that are larger than 10 acres. As part of a study to assess the average mercury contamination in these lakes, a random sample of \\(n=53\\) lakes, an unspecified number of fish were harvested and the average mercury level (in ppm) was calculated for fish in each lake. The goal of the study was to assess if the average mercury concentration was greater than the 1969 EPA “legally actionable level” of 0.5 ppm. # read the Lakes data set Lakes &lt;- read.csv(&#39;http://www.lock5stat.com/datasets/FloridaLakes.csv&#39;) # make a nice picture... dot plots are very similar to histograms # but in this case, my y-axis doen&#39;t make any sense. ggplot(Lakes, aes(x=AvgMercury)) + geom_dotplot() ## `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`. We can calculate mean average mercury level for the \\(n=53\\) lakes Lakes %&gt;% summarise(xbar = mean( AvgMercury )) ## xbar ## 1 0.5271698 The sample mean is greater than \\(0.5\\) but not by too much. Is a true population mean concentration \\(\\mu_{Hg}\\) that is \\(0.5\\) or less incompatible with our observed data? Is our data sufficient evidence to conclude that the average mercury content is greater than \\(0.5\\)? Perhaps the true average mercury content is less than (or equal to) \\(0.5\\) and we just happened to get a random sample that with a mean greater than \\(0.5\\)? The first step in answering these questions is to create an estimate of the sampling distribution of \\(\\bar{x}_{Hg}\\). To do this, we will sample from the approximate population of lakes, which is just many many replicated copies of our sample data. # create the sampling distribution of xbar SamplingDist &lt;- do(10000) * resample(Lakes) %&gt;% summarise(xbar = mean(AvgMercury)) # what columns does the data frame &quot;SamplingDist&quot; have? head(SamplingDist) ## xbar ## 1 0.5111321 ## 2 0.5043396 ## 3 0.5252830 ## 4 0.5362264 ## 5 0.4992453 ## 6 0.5960377 # show a histogram of the sampling distribution of xbar ggplot(SamplingDist, aes(x=xbar)) + geom_histogram() + ggtitle(&#39;Estimated Sampling distribution of xbar&#39; ) 3.2 Quantile-based Confidence Intervals In many cases we have seen, the sampling distribution of a statistic is centered on the parameter we are interested in estimating and is symmetric about that parameterThere are actually several ways to create a confidence interval from the estimated sampling distribution. The method presented here is called the “percentile” method and works when the sampling distribution is symmetric and the estimator we are using is unbiased. For example, we expect that the sample mean \\(\\bar{x}\\) should be a good estimate of the population mean \\(\\mu\\) and the sampling distribution of \\(\\bar{x}\\) should look something like the following. There are two points, (call them \\(L\\) and \\(U\\)) where for our given sample size and population we are sampling from, where we expect that \\(95\\%\\) of the sample means to fall within. That is to say, \\(L\\) and \\(U\\) capture the middle \\(95\\%\\) of the sampling distribution of \\(\\bar{x}\\). These sample means are randomly distributed about the population mean \\(\\mu\\). Given our sample data and sample mean \\(\\bar{x}\\), we can examine how our simulated values of \\(\\bar{x}^{*}\\) vary about \\(\\bar{x}\\). I expect that these simulated sample means \\(\\bar{x}^{*}\\) should vary about \\(\\bar{x}\\) in the same way that \\(\\bar{x}\\) values vary around \\(\\mu\\). Below are three estimated sampling distributions that we might obtain from three different samples and their associated sample means. For each possible sample, we could consider creating the estimated sampling distribution of \\(\\bar{X}\\) and calculating the \\(L\\) and \\(U\\) values that capture the middle \\(95\\%\\) of the estimated sampling distribution. Below are twenty samples, where we’ve calculated this interval for each sample. Most of these intervals contain the true parameter \\(\\mu\\), that we are trying to estimate. In practice, I will only take one sample and therefore will only calculate one sample mean and one interval, but I want to recognize that the method I used to produce the interval (i.e. take a random sample, calculate the mean and then the interval) will result in intervals where only \\(95\\%\\) of those intervals will contain the mean \\(\\mu\\). Therefore, I will refer to the interval as a \\(95\\%\\) confidence interval. After the sample is taken and the interval is calculated, the numbers lower and upper bounds of the confidence interval are fixed. Because \\(\\mu\\) is a constant value and the confidence interval is fixed, nothing is changing. To distinguish between a future random event and the fixed (but unknown) outcome of if I ended up with an interval that contains \\(\\mu\\) and we use the term confidence interval instead of probability interval. # create the sampling distribution of xbar SamplingDist &lt;- do(10000) * resample(Lakes) %&gt;% summarise(xbar=mean(AvgMercury)) # show a histogram of the sampling distribution of xbar ggplot(SamplingDist, aes(x=xbar, y=..density..)) + geom_histogram() + ggtitle(&#39;Estimated Sampling distribution of xbar&#39;) # calculate the 95% confidence interval using middle 95% of xbars quantile( SamplingDist$xbar, probs=c(.025, .975) ) ## 2.5% 97.5% ## 0.4375472 0.6211368 There are several ways to interpret this interval. The process used to calculate this interval (take a random sample, calculate a statistic, repeatedly resample, and take the middle \\(95\\%\\)) is a process that results in an interval that contains the parameter of interest on \\(95\\%\\) of the samples we could have collected, however we don’t know if the particular sample we collected and its resulting interval of \\(\\left(0.44,\\,0.62\\right)\\) is one of the intervals containing \\(\\mu\\). We are \\(95\\%\\) confident that \\(\\mu\\) is in the interval \\(\\left(0.44,\\,0.62\\right)\\). This is delightfully vague and should be interpreted as a shorter version of the previous interpretation. The interval \\(\\left(0.44,\\,0.62\\right)\\) is the set of values of \\(\\mu\\) that are consistent with the observed data at the \\(0.05\\) threshold of statistical significance for a two-sided hypothesis test Example: Fuel Economy Suppose we have data regarding fuel economy of \\(5\\) new vehicles of the same make and model and we wish to test if the observed fuel economy is consistent with the advertised \\(31\\) mpg at highway speeds. We the data are CarMPG &lt;- data.frame( ID=1:5, mpg = c(31.8, 32.1, 32.5, 30.9, 31.3) ) CarMPG %&gt;% summarise( xbar=mean(mpg) ) ## xbar ## 1 31.72 We will use the sample mean to assess if the sample fuel efficiency is consistent with the advertised number. Because these cars could be considered a random sample of all new cars of this make, we will create the estimated sampling distribution using the bootstrap resampling of the data. SamplingDist &lt;- do(10000) * resample(CarMPG) %&gt;% summarise(xbar=mean(mpg)) # show a histogram of the sampling distribution of xbar ggplot(SamplingDist, aes(x=xbar)) + geom_histogram() + ggtitle(&#39;Estimated Sampling distribution of xbar&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # calculate the 95% confidence interval using middle 95% of xbars quantile( SamplingDist$xbar, probs=c(.025, .975) ) ## 2.5% 97.5% ## 31.24 32.20 We see that the \\(95\\%\\) confidence interval is \\(\\left(31.2,\\,32.2\\right)\\) and does not actually contain the advertised \\(31\\) mpg. However, I don’t think we would object to a car manufacturer selling us a car that is better than advertised. Example: Pulse Rate of College Students In the package Lock5Data, the dataset GPAGender contains information taken from undergraduate students in an Introductory Statistics course. This is a convenience sample, but could be considered representative of students at that university. One of the covariates measured was the students pulse rate and we will use this to create a confidence interval for average pulse of students at that university. First we’ll look at the raw data. library(Lock5Data) # load the package data(GPAGender) # from the package, load the dataset # Now a nice histogram ggplot(GPAGender, aes(x=Pulse, y=..density..)) + geom_histogram(binwidth=2) + ggtitle(&#39;Sample Data&#39;) It is worth noting this was supposed to be measuring resting heart rates, but there are two students had extremely high pulse rates and six with extremely low rates. The two high values are approximately what you’d expect from someone currently engaged in moderate exercise and the low values are levels we’d expect from highly trained endurance athletes. # Summary Statistics GPAGender %&gt;% summarise(xbar = mean(Pulse), StdDev = sd(Pulse)) ## xbar StdDev ## 1 69.90379 12.08569 So the sample mean is \\(\\bar{x}=69.9\\) but how much should we expect our sample mean to vary from sample to sample when our sample size is \\(n=343\\) people? We’ll estimate the sampling distribution of \\(\\bar{X}\\) using the bootstrap. # Create the bootstrap replicates SampDist &lt;- do(10000) * resample(GPAGender) %&gt;% summarise(xbar = mean(Pulse)) ggplot(SampDist, aes(x=xbar, y=..density..)) + geom_histogram(binwidth=.2) + ggtitle(&#39;Sampling Distribution of Mean(Pulse)&#39;) Just by sampling variability, we expect the sampling mean \\(\\bar{X}\\) to vary from approximately 68 to 72. The appropriate quantiles for a \\(95\\%\\) bootstrap confidence interval are actually quantile( SampDist$xbar, probs=c(0.025, 0.975) ) ## 2.5% 97.5% ## 68.65015 71.18958 3.3 Exercises For several of these exercises, we will use data sets from the R package Lock5Data, which greatly contributed to the pedagogical approach of these notes. Install the package from CRAN using the RStudio point-and-click interface Tools -&gt; Install Packages…. Load the dataset BodyTemp50 from the Lock5Data package. This is a dataset of 50 healthy adults. Unfortunately the documentation doesn’t give how the data was collected, but for this problem we’ll assume that it is a representative sample of healthy US adults. library(Lock5Data) data( BodyTemp50 ) ?BodyTemp50 One of the columns of this dataset is the Pulse of the 50 data points, which is the number of heartbeats per minute. Create a histogram of the observed pulse values. Comment on the graph and aspects of the graph that might be of scientific interest. Calculate the sample mean \\(\\bar{x}\\) and sample standard deviation \\(s\\) of the pulses. Create a dataset of 10000 bootstrap replicates of \\(\\bar{x}^{*}\\). Create a histogram of the bootstrap replicates. Calculate the mean and standard deviation of this distribution. Notice that the standard deviation of the distribution is often called the Standard Error of \\(\\bar{x}\\) and we’ll denote it as \\(\\sigma_{\\bar{x}}\\). Using the bootstrap replicates, create a 95% confidence interval for \\(\\mu\\) , the average adult heart rate. Calculate the interval \\[\\left(\\bar{x}-2\\cdot\\hat{\\sigma}_{\\bar{x}}\\,,\\,\\;\\;\\bar{x}+2\\cdot\\hat{\\sigma}_{\\bar{x}}\\right)\\] and comment on its similarity to the interval you calculated in part (e). Load the dataset EmployedACS from the Lock5Data package. This is a dataset drawn from American Community Survey results which is conducted monthly by the US Census Bureau and should be representative of US workers. The column HoursWk represents the number of hours worked per week. Create a histogram of the observed hours worked. Comment on the graph and aspects of the graph that might be of scientific interest. Calculate the sample mean \\(\\bar{x}\\) and sample standard deviation \\(s\\) of the worked hours per week. Create a dataset of 10000 bootstrap replicates of \\(\\bar{x}^{*}\\). Create a histogram of the bootstrap replicates. Calculate the mean and standard deviation of this distribution. Notice that the standard deviation of the distribution is often called the Standard Error of \\(\\bar{x}\\) and we’ll denote it as \\(\\sigma_{\\bar{x}}\\). Using the bootstrap replicates, create a 95% confidence interval for \\(\\mu\\), the average worked hours per week. Calculate the interval \\[\\left(\\bar{x}-2\\cdot\\hat{\\sigma}_{\\bar{x}}\\,,\\,\\;\\;\\bar{x}+2\\cdot\\hat{\\sigma}_{\\bar{x}}\\right)\\] and comment on its similarity to the interval you calculated in part (e). "],
["4-sampling-distribution-of-barx.html", "Chapter 4 Sampling Distribution of \\(\\bar{X}\\) 4.1 Enlightening Example 4.2 Mathematical details 4.3 Distribution of \\(\\bar{X}\\) 4.4 Central Limit Theorem 4.5 Exercises", " Chapter 4 Sampling Distribution of \\(\\bar{X}\\) # load the ggplot2 and dplyr packages... which I use constantly. library(ggplot2) library(dplyr) # other packages I&#39;ll only use occasionally so instead of loading the # whole package, I&#39;ll just do packageName::functionName() when I use # the function. In the previous chapter, we using bootstrapping to estimate the sampling distribution of \\(\\bar{X}\\). We then used this bootstrap distribution to calculate a confidence interval for the population mean. We noticed that the sampling distribution of \\(\\bar{X}\\) almost always looked like a normal distribution. Prior to the advent of modern computing, statisticians used a theoretical approximation known as the Central Limit Theorem (CLT). Even today, statistical procedures based on the CLT are widely used and often perform as the corresponding resampling technique. In this chapter we’ll lay the theoretical foundations for the CLT as well as introduce computation 4.1 Enlightening Example Suppose we are sampling from a population that has a mean of \\(\\mu=5\\) and is skewed. For this example, I’ll use a Chi-squared distribution with parameter \\(\\nu=5\\). # Population is a Chi-sq distribution with df=5 PopDist &lt;- data.frame(x = seq(0,20,length=10000)) %&gt;% mutate(density=dchisq(x,df=5)) ggplot(PopDist, aes(x=x, y=density)) + geom_area(fill=&#39;salmon&#39;) + ggtitle(&#39;Population Distribution&#39;) We want to estimate the mean \\(\\mu\\) and take a random sample of \\(n=5\\). Lets do this a few times and notice that the sample mean is never exactly 5, but is a bit off from that. n &lt;- 5 # Our Sample Size! mosaic::do(3) * { Sample.Data &lt;- data.frame( x = rchisq(n,df=5) ) Sample.Data %&gt;% summarise( xbar = mean(x) ) } ## xbar ## 1 3.582744 ## 2 4.105642 ## 3 3.324081 n &lt;- 5 SampDist &lt;- mosaic::do(10000) * { Sample.Data &lt;- data.frame( x = rchisq(n,df=5) ) Sample.Data %&gt;% summarise( xbar = mean(x) ) } We will compare the population distribution to the sampling distribution graphically. ggplot() + geom_area(data=PopDist, aes(x=x, y=density), fill=&#39;salmon&#39;) + geom_histogram(data=SampDist, aes(x=xbar, y=..density..), binwidth=.1, alpha=.6) # alpha is the opacity of the layer From the histogram of the sample means, we notice three things: The sampling distribution of \\(\\bar{X}\\) is centered at the population mean \\(\\mu\\). The sampling distribution of \\(\\bar{X}\\) has less spread than the population distribution. The sampling distribution of \\(\\bar{X}\\) is less skewed than the population distribution. 4.2 Mathematical details 4.2.1 Probability Rules for Expectations and Variances Claim: For random variables \\(X\\) and \\(Y\\) and constant \\(a\\) the following statements hold: \\[E\\left(aX\\right) = aE\\left(X\\right)\\] \\[Var\\left(aX\\right) = a^{2}Var\\left(X\\right)\\] \\[E\\left(X+Y\\right) = E\\left(X\\right)+E\\left(Y\\right)\\] \\[E\\left(X-Y\\right) = E\\left(X\\right)-E\\left(Y\\right)\\] \\[Var\\left(X\\pm Y\\right) = Var\\left(X\\right)+Var\\left(Y\\right)\\;\\textrm{if X,Y are independent}\\] Proving these results is relatively straight forward and is done in almost all introductory probability text books. 4.2.2 Mean and Variance of the Sample Mean We have been talking about random variables drawn from a known distribution and being able to derive their expected values and variances. We now turn to the mean of a collection of random variables. Because sample values are random, any function of them is also random. So even though the act of calculating a mean is not a random process, the numbers that are feed into the algorithm are random. Thus the sample mean will change from sample to sample and we are interested in how it varies. Using the rules we have just confirmed, it is easy to calculate the expectation and variance of the sample mean. Given a sample \\(X_{1},X_{2},\\dots,X_{n}\\) of observations where all the observations are independent of each other and all the observations have expectation \\(E\\left[X_{i}\\right]=\\mu\\) and variance \\(Var\\left[X_{i}\\right]=\\sigma^{2}\\) then \\[\\begin{aligned}E\\left[\\bar{X}\\right] &amp;= E\\left[\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right] \\\\ &amp;= \\frac{1}{n}E\\left[\\sum_{i=1}^{n}X_{i}\\right] \\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^{n}E\\left[X_{i}\\right] \\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^{n}\\mu \\\\ &amp;= \\frac{1}{n}\\,n\\mu \\\\ &amp;= \\mu\\end{aligned}\\] and \\[\\begin{aligned} Var\\left[\\bar{X}\\right] &amp;= Var\\left[\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right] \\\\ &amp;= \\frac{1}{n^{2}}Var\\left[\\sum_{i=1}^{n}X_{i}\\right] \\\\ &amp;= \\frac{1}{n^{2}}\\sum_{i=1}^{n}Var\\left[X_{i}\\right] \\\\ &amp;= \\frac{1}{n^{2}}\\sum_{i=1}^{n}\\sigma^{2} \\\\ &amp;= \\frac{1}{n^{2}}\\,n\\sigma^{2} \\\\ &amp;= \\frac{\\sigma^{2}}{n} \\end{aligned}\\] Notice that the sample mean has the same expectation as the original distribution that the samples were pulled from, but it has a smaller variance! So the sample mean is an unbiased estimator of the population mean \\(\\mu\\) and the average distance of the sample mean to the population mean decreases as the sample size becomes larger. 4.3 Distribution of \\(\\bar{X}\\) If the samples were drawn from a normal distribution If \\(X_{i}\\stackrel{iid}{\\sim}N\\left(\\mu,\\sigma^{2}\\right)\\) then it is well known (and proven in most undergraduate probability classes) that \\(\\bar{X}\\) is also normally distributed with a mean and variance that were already established. That is \\[\\bar{X}\\sim N\\left(\\mu_{\\bar{X}}=\\mu,\\;\\sigma_{\\bar{X}}^{2}=\\frac{\\sigma^{2}}{n}\\right)\\] Notation: Because the expectations of \\(X\\) and \\(\\bar{X}\\) are the same, I could drop the subscript for the expectation of \\(\\bar{X}\\) but it is sometimes helpful to be precise. Because the variances are different we will use \\(\\sigma_{\\bar{X}}\\) to denote the standard deviation of \\(\\bar{X}\\) and \\(\\sigma_{\\bar{X}}^{2}\\) to denote variance of \\(\\bar{X}\\). If there is no subscript, we are referring to the population parameter of the distribution from which we taking the sample from. Exercise: A researcher measures the wingspan of a captured Mountain Plover three times. Assume that each of these \\(X_{i}\\) measurements comes from a \\(N\\left(\\mu=6\\textrm{ inches},\\,\\sigma^{2}=1^{2}\\textrm{ inch}\\right)\\) distribution. What is the probability that the first observation is greater than 7? \\[\\begin{aligned}P\\left(X\\ge7\\right) &amp;= P\\left(\\frac{X-\\mu}{\\sigma}\\ge\\frac{7-6}{1}\\right) \\\\ &amp;= P\\left(Z\\ge1\\right) \\\\ &amp;= 0.1587 \\end{aligned}\\] What is the distribution of the sample mean? \\[\\bar{X}\\sim N\\left(\\mu_{\\bar{X}}=6,\\,\\;\\sigma_{\\bar{X}}^{2}=\\frac{1^{2}}{3}\\right)\\] What is the probability that the sample mean is greater than 7? \\[\\begin{aligned}P\\left(\\bar{X}\\ge7\\right) &amp;= P\\left(\\frac{\\bar{X}-\\mu_{\\bar{X}}}{\\sigma_{\\bar{X}}}\\ge\\frac{7-6}{\\sqrt{\\frac{1}{3}}}\\right) \\\\ &amp;= P\\left(Z\\ge\\sqrt{3}\\right) \\\\ &amp;= P\\left(Z\\ge1.73\\right) \\\\ &amp;= 0.0418 \\end{aligned}\\] Example: Suppose that the weight of an adult black bear is normally distributed with standard deviation \\(\\sigma=50\\) pounds. How large a sample do I need to take to be \\(95\\%\\) certain that my sample mean is within \\(10\\) pounds of the true mean \\(\\mu\\)? So we want \\[\\left|\\bar{X}-\\mu\\right| \\le 10\\] which we rewrite as \\[-10 \\le\\bar{X}-\\mu_{\\bar{X}}\\le 10\\] \\[\\frac{-10}{\\left(\\frac{50}{\\sqrt{n}}\\right)} \\le\\frac{\\bar{X}-\\mu_{\\bar{X}}}{\\sigma_{\\bar{X}}}\\le \\frac{10}{\\left(\\frac{50}{\\sqrt{n}}\\right)}\\] \\[\\frac{-10}{\\left(\\frac{50}{\\sqrt{n}}\\right)} \\le Z\\le \\frac{10}{\\left(\\frac{50}{\\sqrt{n}}\\right)}\\] Next we look in our standard normal table to find a \\(z\\)-value such that \\(P\\left(-z\\le Z\\le z\\right)=0.95\\) and that value is \\(z=1.96\\). data &lt;- data.frame( z= seq(-3, 3, length=1000) ) %&gt;% mutate( y = dnorm(z) ) ggplot(data, aes(x=z, y=y)) + geom_line() + geom_area( data = data %&gt;% filter(abs(z) &lt;= 1.96), fill=&#39;grey&#39;, alpha=.7) + geom_text( x=0, y=.2, label=&#39;95%&#39;) So all we need to do is solve the following equation for \\(n\\) \\[1.96 = \\frac{10}{ \\left( \\frac{50}{\\sqrt{n}} \\right) }\\] \\[\\frac{1.96}{10}\\left(50\\right) = \\sqrt{n}\\] \\[96 \\approx n\\] 4.4 Central Limit Theorem I know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the “Law of Frequency of Error”. The law would have been personified by the Greeks and deified, if they had known of it. It reigns with serenity and in complete self-effacement, amidst the wildest confusion. The huger the mob, and the greater the apparent anarchy, the more perfect is its sway. It is the supreme law of Unreason. Whenever a large sample of chaotic elements are taken in hand and marshaled in the order of their magnitude, an unsuspected and most beautiful form of regularity proves to have been latent all along. - Sir Francis Galton (1822-1911) It was not surprising that the average of a number of normal random variables is also a normal random variable. Because the average of a number of binomial random variables cannot be binomial since the average could be something besides a \\(0\\) or \\(1\\) and the average of Poisson random variables does not have to be an integer. The question arises, what can we say the distribution of the sample mean if the data comes from a non-normal distribution? The answer is quite a lot! Provided the distribution sample from has a non-infinite variance and we have a sufficient sample size. Central Limit Theorem Let \\(X_{1},\\dots X_{n}\\) be independent observations collected from a distribution with expectation \\(\\mu\\) and variance \\(\\sigma^{2}\\). Then the distribution of \\(\\bar{X}\\) converges to a normal distribution with expectation \\(\\mu\\) and variance \\(\\sigma^{2}/n\\) as \\(n\\rightarrow\\infty\\). In practice this means that if \\(n\\) is large (usually \\(n&gt;30\\) is sufficient), then \\[\\bar{X}\\stackrel{\\cdot}{\\sim}N\\left(\\mu_{\\bar{X}}=\\mu,\\,\\,\\,\\sigma_{\\bar{X}}^{2}=\\frac{\\sigma^{2}}{n}\\right)\\] So what does this mean? Variables that are the sum or average of a bunch of other random variables will be close to normal. Example: human height is determined by genetics, pre-natal nutrition, food abundance during adolescence, etc. Similar reasoning explains why the normal distribution shows up surprisingly often in natural science. With sufficient data, the sample mean will have a known distribution and we can proceed as if the sample mean came from a normal distribution. Example: Suppose the waiting time from order to delivery at a fast-food restaurant is a exponential random variable with rate \\(\\lambda=1/2\\) minutes and so the expected wait time is \\(2\\) minutes and the variance is \\(4\\) minutes. What is the approximate probability that we observe a sample of size \\(n=40\\) with a mean time greater than \\(2.5\\) minutes? \\[\\begin{aligned}P\\left(\\bar{X}\\ge2.5\\right) &amp;= P\\left(\\frac{\\bar{X}-\\mu_{\\bar{X}}}{\\sigma_{\\bar{X}}}\\ge\\frac{2.5-\\mu_{\\bar{X}}}{\\sigma_{\\bar{X}}}\\right) \\\\ &amp;\\approx P\\left(Z\\ge\\frac{2.5-2}{\\frac{2}{\\sqrt{40}}}\\right) \\\\ &amp;= P\\left(Z\\ge1.58\\right) \\\\ &amp;= 0.0571 \\end{aligned}\\] # Answer obtained via simulation SampDist &lt;- mosaic::do(10000) *{ # make 10,000 Sample &lt;- data.frame( x= rexp(n=40, rate=1/2 ) ) # simulated xbar Sample %&gt;% summarise( xbar = mean( x ) ) # values } SampDist %&gt;% # What proportion of those mutate(Greater = ifelse(xbar &gt;= 2.5, 1, 0)) %&gt;% # xbar values are summarise( ProportionGreater = mean(Greater) ) # greater than 2.5? ## ProportionGreater ## 1 0.0704 Summary Often we have sampled \\(n\\) elements from some population \\(Y_{1},Y_{2},\\dots,Y_{n}\\) independently and \\(E\\left(Y_{i}\\right)=\\mu\\) and \\(Var\\left(Y_{i}\\right)=\\sigma^{2}\\) and we want to understand the distribution of the sample mean, that is we want to understand how the sample mean varies from sample to sample. \\(E\\left(\\bar{Y}\\right)=\\mu\\). That states that the distribution of the sample mean will centered at \\(\\mu\\). We expect to sometimes take samples where the sample mean is higher than \\(\\mu\\) and sometimes less than \\(\\mu\\), but the average underestimate is the same magnitude as the average overestimate. \\(Var\\left(\\bar{Y}\\right)=\\frac{\\sigma^{2}}{n}\\). This states that as our sample size increases, we trust the sample mean to be close to \\(\\mu\\). The larger the sample size, the greater our expectation that the \\(\\bar{Y}\\) will be close to \\(\\mu\\). If \\(Y_{1},Y_{2},\\dots,Y_{n}\\) were sampled from a \\(N\\left(\\mu,\\sigma^{2}\\right)\\) distribution then \\(\\bar{Y}\\) is normally distributed. \\[\\bar{Y} \\sim N\\left(\\mu_{\\bar{Y}}=\\mu,\\;\\;\\sigma_{\\bar{Y}}^{2}=\\frac{\\sigma^{2}}{n}\\right)\\] If \\(Y_{1},Y_{2},\\dots,Y_{n}\\) were sampled from a distribution that is not normal but has mean \\(\\mu\\) and variance \\(\\sigma^{2}\\), and our sample size is large, then \\(\\bar{Y}\\) is approximately normally distributed. \\[\\bar{Y} \\stackrel{\\cdot}{\\sim} N\\left(\\mu_{\\bar{Y}}=\\mu,\\;\\;\\sigma_{\\bar{Y}}^{2}=\\frac{\\sigma^{2}}{n}\\right)\\] 4.5 Exercises Suppose that the amount of fluid in a small can of soda can be well approximated by a Normal distribution. Let \\(X\\) be the amount of soda (in milliliters) in a single can and \\(X\\sim N\\left(\\mu=222,\\;\\sigma=5\\right)\\). \\(P\\left(X&gt;230\\right)=\\) Suppose we take a random sample of 6 cans such that the six cans are independent. What is the expected value of the mean of those six cans? In other words, what is \\(E\\left(\\bar{X}\\right)\\)? What is \\(Var\\left(\\bar{X}\\right)\\)? (Recall we denote this as \\(\\sigma_{\\bar{X}}^{2}\\)) What is the standard deviation of \\(\\bar{X}\\)? (Recall we denote this as \\(\\sigma_{\\bar{X}}\\)) What is the probability that the sample mean will be greater than 230 ml? That is, find \\(P\\left(\\bar{X}&gt;230\\right)\\). Suppose that the number of minutes that I spend waiting for my order at Big Foot BBQ can be well approximated by a Normal distribution with mean \\(\\mu=10\\) minutes and standard deviation \\(\\sigma=1.5\\) minutes. Tonight I am planning on going to Big Foot BBQ. What is the probability I have to wait less than 9 minutes? Over the next month, I’ll visit Big Foot BBQ 5 times. What is the probability that the mean waiting time of those 5 visits is less than 9 minutes? (This assumes independence of visits but because I don’t hit the same restaurant the same night each week, this assumption is probably ok.) A bottling company uses a machine to fill bottles with a tasty beverage. The bottles are advertised to contain 300 milliliters (ml), but in reality the amount varies according to a normal distribution with mean \\(\\mu=298\\) ml and standard deviation \\(\\sigma=3\\) ml. (For this problem, we’ll assume \\(\\sigma\\) is known and carry out the calculations accordingly). What is the probability that a randomly chosen bottle contains less than 296 ml? Given a simple random sample of size \\(n=6\\) bottles, what is the probability that the sample mean is less than \\(296\\) ml? What is the probability that a single bottle is filled within \\(1\\) ml of the true mean \\(\\mu=298\\) ml? Hint: Draw the distribution and shade in what probability you want… then convert that to a question about standard normals. To find the answer using a table or R, you need to look up two values and perform a subtraction. What is the probability that the mean of \\(10\\) randomly selected bottles is within \\(1\\) ml of the mean? What about the mean of a sample of \\(n=100\\) bottles? If a sample of size \\(n=50\\) has a sample mean of \\(\\bar{x}=298\\), should this be evidence that the filling machine is out of calibration? i.e., assuming the machine has a mean fill amount of \\(\\mu=300\\) and \\(\\sigma=3\\), what is \\(P\\left(\\bar{X}\\le298\\right)\\)? "],
["5-confidence-intervals-for-mu.html", "Chapter 5 Confidence Intervals for \\(\\mu\\) 5.1 Asymptotic result (\\(\\sigma\\) known) 5.2 Asymptotoic result (\\(\\sigma\\) unknown) 5.3 Sample Size Selection 5.4 Exercises", " Chapter 5 Confidence Intervals for \\(\\mu\\) library(ggplot2) library(dplyr) library(mosaic) 5.1 Asymptotic result (\\(\\sigma\\) known) We know that our sample mean \\(\\bar{x}\\), should be close to the population mean \\(\\mu\\). So when giving a region of values for \\(\\mu\\) that are consistent with the observed data, we would expect our CI formula to be something like \\(\\left(\\bar{x}-d,\\;\\bar{x}+d\\right)\\) for some value \\(d\\). That value of \\(d\\) should be small if our sample size is big, representing our faith that a large amount of data should result in a statistic that is very close to the true value of \\(\\mu\\). Recall that if our data \\(X_{i}\\sim N\\left(\\mu,\\,\\sigma^{2}\\right)\\) or our sample size was large enough, then we know \\[\\bar{X}\\sim N\\left(\\mu,\\,\\;\\sigma_{\\bar{X}}^{2}=\\frac{\\sigma^{2}}{n}\\right)\\] or is approximately so. Doing a little re-arranging, we see that \\[\\frac{\\bar{X}-\\mu}{\\left(\\frac{\\sigma}{\\sqrt{n}}\\right)}\\sim N\\left(0,1\\right)\\] So if we take the 0.025 and 0.975 quantiles of the normal distribution, which are \\(z_{0.025}=-1.96\\) and \\(z_{0.975}=1.96\\), we could write \\[\\begin{aligned}0.95 &amp;= P\\left[ -1.96\\le\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\le1.96 \\right] \\\\ &amp;= P\\left[ -1.96\\left(\\frac{\\sigma}{\\sqrt{n}}\\right)\\le\\bar{X}-\\mu\\le1.96\\left(\\frac{\\sigma}{\\sqrt{n}}\\right) \\right] \\\\ &amp;= P\\left[ \\bar{X}-1.96\\left(\\frac{\\sigma}{\\sqrt{n}}\\right)\\le\\mu\\le\\bar{X}+1.96\\left(\\frac{\\sigma}{\\sqrt{n}}\\right) \\right] \\end{aligned}\\] Which suggests that a reasonable 95% Confidence Interval for \\(\\mu\\) is \\[\\bar{x}\\pm1.96\\left(\\frac{\\sigma}{\\sqrt{n}}\\right)\\] In general for a \\(\\left(1-\\alpha\\right)\\cdot100\\%\\) confidence interval, we would use the formula \\(\\bar{x}\\pm z_{1-\\alpha/2}\\left(\\frac{\\sigma}{\\sqrt{n}}\\right)\\). Notice that I could write the formula using \\(z_{\\alpha/2}\\) instead of \\(z_{1-\\alpha/2}\\) because the normal distribution is symmetric about 0 and we are subtracting and adding the same quantity to \\(\\bar{x}\\). The interpretation of a confidence interval is that over repeated sampling, \\(100(1-\\alpha)\\%\\) of the resulting intervals will contain the population mean \\(\\mu\\) but we don’t know if the interval we have actually observed is one of the good intervals that contains the mean \\(\\mu\\) or not. Because this is quite the mouthful, we will say “we are \\(100\\left(1-\\alpha\\right)\\%\\) confident that the observed interval contains the mean \\(\\mu\\).” Example: Suppose a bottling facility has a machine that supposedly fills bottles to 300 milliliters (ml) and is known to have a standard deviation of \\(\\sigma=3\\) ml. However, the machine occasionally gets out of calibration and might be consistently overfilling or under-filling bottles. To discover if the machine is calibrated correctly, we take a random sample of \\(n=40\\) bottles and observe the mean amount filled was \\(\\bar{x}=299\\) ml. We calculate a \\(95\\%\\) confidence interval (CI) to be \\[\\begin{aligned} \\bar{x} &amp;\\pm z_{1-\\alpha/2}\\left(\\frac{\\sigma}{\\sqrt{n}}\\right)\\\\ 299 &amp;\\pm 1.96\\left(\\frac{3}{\\sqrt{40}}\\right) \\\\ 299 &amp;\\pm 0.93 \\end{aligned}\\] and conclude that we are \\(95\\%\\) confident that the that the true mean fill amount is in \\(\\left[298.07,299.93\\right]\\) and that the machine has likely drifted off calibration. 5.2 Asymptotoic result (\\(\\sigma\\) unknown) It is unrealistic to expect that we know the population variance \\(\\sigma^{2}\\) but do not know the population mean \\(\\mu\\). So in calculations that involve \\(\\sigma\\), we want to use the sample standard deviation \\(s\\) instead. Our previous results about confidence intervals assumed that \\(\\bar{X}\\sim N\\left(\\mu,\\frac{\\sigma^{2}}{n}\\right)\\) (or is approximately so) and therefore \\[\\frac{\\bar{X}-\\mu}{\\sqrt{\\frac{\\sigma^{2}}{n}}}\\sim N\\left(0,1\\right)\\] I want to just replace \\(\\sigma^{2}\\) with \\(S^{2}\\) but the sample variance \\(S^{2}\\) is also a random variable and incorporating it into the standardization function might affect the distribution. \\[\\frac{\\bar{X}-\\mu}{\\sqrt{\\frac{S^{2}}{n}}}\\sim\\;???\\] Unfortunately this substitution of \\(S^{2}\\) for \\(\\sigma^{2}\\) comes with a cost and this quantity is not normally distributed. Instead it has a \\(t\\)-distribution with \\(n-1\\) degrees of freedom. However as the sample size increases and \\(S^{2}\\) becomes a more reliable estimator of \\(\\sigma^{2}\\), this penalty should become smaller. The \\(t\\)-distribution is named after William Gosset who worked at Guinness Brewing and did work with small sample sizes in both the brewery and at the farms that supplied the barley. Because Guinness prevented its employees from publishing any of their work, he published under the pseudonym Student. Notice that as the sample size increases, the t-distribution gets closer and closer to the normal distribution. From here on out, we will use the following standardization formula: \\[\\frac{\\bar{X}-\\mu}{\\frac{S}{\\sqrt{n}}}\\sim\\;t_{n-1}\\] and emphasize that this formula is valid if the sample observations came from a population with a normal distribution or if the sample size is large enough for the Central Limit Theorem to imply that \\(\\bar{X}\\) is approximately normally distributed. Substituting the sample standard deviation into the confidence interval formula, we also substitute a t-quantile for the standard normal quantile. We will denote \\(t_{n-1}^{1-\\alpha/2}\\) as the \\(1-\\alpha/2\\) quantile of a \\(t\\)-distribution with \\(n-1\\) degrees of freedom. Therefore we will use the following formula for the calculation of \\(100\\left(1-\\alpha\\right)\\%\\) confidence intervals for the mean \\(\\mu\\): \\[\\bar{x}\\pm t_{n-1}^{1-\\alpha/2}\\left(\\frac{s}{\\sqrt{n}}\\right)\\] Notation: We will be calculating confidence intervals for the rest of the course and it is useful to recognize the skeleton of a confidence interval formula. The basic form is always the same \\[Estimate\\;\\pm\\,t_{df}^{1-\\alpha/2}\\,\\,Standard\\,Error\\left(\\,Estimate\\,\\right)\\] In our current problem, \\(\\bar{x}\\) is our estimate of \\(\\mu\\) and the estimated standard deviation (which is commonly called the standard error) is \\(s/\\sqrt{n}\\) and the appropriate degrees of freedom are \\(df=n-1\\). Example: Suppose we are interested in calculating a \\(95\\%\\) confidence interval for the mean weight of adult black bears. We collect a random sample of \\(40\\) individuals (large enough for the CLT to kick in) and observe the following data: bears &lt;- data.frame(weight = c(306, 446, 276, 235, 295, 302, 374, 339, 624, 266, 497, 384, 429, 497, 224, 157, 248, 349, 388, 391, 266, 230, 621, 314, 344, 413, 267, 380, 225, 418, 257, 466, 230, 548, 277, 354, 271, 369, 275, 272)) xbar &lt;- mean(bears$weight) s &lt;- sd( bears$weight) cbind(xbar, s) ## xbar s ## [1,] 345.6 108.8527 Notice that the data do not appear to come from a normal distribution, but a slightly heavier right tail. We’ll plot the histogram of data along with a normal distribution with the same mean and standard deviation as our data. normal.data &lt;- data.frame(weight=seq(100,700,length=1000)) %&gt;% mutate( y = dnorm(weight, mean=xbar, sd=s)) ggplot() + labs(y=&#39;density&#39;) + geom_area( data=normal.data, aes(x=weight, y=y), fill=&#39;light blue&#39; ) + geom_histogram(data=bears, aes(x=weight, y=..density..), binwidth=30, alpha=.6) The observed sample mean is \\(\\bar{x}=345.6\\) pounds and a sample standard deviation \\(s=108.8527\\) pounds. Because we want a \\(95\\%\\) $confidence interval \\(\\alpha=0.05\\). Using t-tables or the following R code qt(.975, df=39) ## [1] 2.022691 we find that \\(t_{n-1}^{1-\\alpha/2}=2.022691\\). Therefore the \\(95\\%\\) confidence interval is \\[\\bar{x} \\pm t_{n-1}^{1-\\alpha/2}\\left(\\frac{s}{\\sqrt{n}}\\right)\\] \\[345.6 \\pm 2.022691\\left(\\frac{108.8527}{\\sqrt{40}}\\right)\\] \\[345.6 \\pm 34.8\\] or \\(\\left(310.8, \\, 380.4\\right)\\) which is interpreted as “We are 95% confident that the true mean \\(\\mu\\) is in this interval” which is shorthand for “The process that resulted in this interval (taking a random sample, and then calculating an interval using the algorithm presented) will result in intervals such that 95% of them contain the mean \\(\\mu\\), but we cannot know of this particular interval is one of the good ones or not.” We can wonder how well this interval matches up with the interval we would have gotten if we had used the bootstrap method to create a confidence interval for \\(\\mu\\). In this case, where the sample size \\(n\\) is relatively large, the Central Limit Theorem is certainly working and the distribution of the sample mean certainly looks fairly normal. SampDist &lt;- mosaic::do(10000) * resample(bears) %&gt;% summarise(xbar=mean(weight)) ggplot(SampDist, aes(x=xbar, y=..density..)) + geom_histogram() Grabbing the appropriate quantiles from the bootstrap estimate of the sampling distribution, we see that the bootstrap \\(95\\%\\) confidence interval matches up will with the confidence interval we obtained from asymptotic theory. quantile( SampDist$xbar, probs=c(0.025, 0.975) ) ## 2.5% 97.5% ## 313.1750 380.4756 Example: Assume that the percent of alcohol in casks of whisky is normally distributed. From the last batch of casks produced, the brewer samples \\(n=5\\) casks and wants to calculate a \\(90\\%\\) confidence interval for the mean percent alcohol in the latest batch produced. The sample mean was \\(\\bar{x}=55\\) percent and the sample standard deviation was \\(s=4\\) percent. \\[\\bar{x} \\pm t_{n-1}^{1-\\alpha/2}\\left(\\frac{s}{\\sqrt{n}}\\right)\\] qt( 1 - .1/2, df=4) # 1-(.1)/2 = 1-.05 = .95 ## [1] 2.131847 \\[55 \\pm 2.13\\left(\\frac{4}{\\sqrt{5}}\\right)\\] \\[55 \\pm 3.8\\] Question: If we wanted a \\(95\\%\\) confidence interval, would it have been wider or narrower? Question: If this interval is too wide to be useful, what could we do to make it smaller? 5.3 Sample Size Selection Often a researcher is in the position of asking how many sample observations are necessary to achieve a specific width of confidence interval. Let the margin of error, which we denote \\(ME\\), be the half-width desired (so the confidence interval would be \\(\\bar{x}\\pm ME\\)). So given the desired confidence level, and if we know \\(\\sigma\\), then we can calculate the necessary number of samples to achieve a particular \\(ME\\). To do this calculation, we must also have some estimate of the population standard deviation \\(\\sigma\\). \\[ME=z_{1-\\alpha/2}\\left(\\frac{\\sigma}{\\sqrt{n}}\\right)\\] and therefore \\[n\\approx\\left[z_{1-\\alpha/2}\\left(\\frac{\\sigma}{ME}\\right)\\right]^{2}\\] Notice that because \\[n\\propto\\left[\\frac{1}{ME}\\right]^{2}\\] then if we want a margin of error that is twice as precise (i.e. the CI is half as wide) then we need to quadruple our sample size! Second, this result requires having some knowledge of \\(\\sigma\\). We could acquire an estimate through: 1. a literature search 2. a pilot study 3. expert opinion. A researcher is interested in estimating the mean weight of an adult elk in Yellowstone’s northern herd after the winter and wants to obtain a \\(90\\%\\) confidence interval with a half-width \\(ME=10\\) pounds. Using prior collection data from the fall harvest (road side checks by game wardens), the researcher believes that \\(\\sigma=60\\) lbs is a reasonable standard deviation number to use. \\[\\begin{aligned} n &amp;\\approx \\left[ z_{0.95} \\left(\\frac{\\sigma}{ME}\\right)\\right]^{2} \\\\ &amp;= \\left[1.645\\left(\\frac{60}{10}\\right)\\right]^{2} \\\\ &amp;= 97.41 \\end{aligned}\\] Notice that I don’t bother using the \\(t\\)-distribution in this calculations because because I am assuming that \\(\\sigma\\) is known. While this is a horrible assumption, the difference between using a \\(t\\) quantile instead of \\(z\\) quantile is small and what really matters is how good the estimate of \\(\\sigma\\) is. As with many things, the quality of the input values is reflected in the quality of the output. Typically this sort of calculation is done with only a rough estimate of \\(\\sigma\\) and therefore I would subsequently regard the resulting sample size \\(n\\) as an equally rough estimate. We could be a bit more precise and use the \\(t\\)-quantile, but because the degrees of freedom depend on \\(n\\) as well, then we would have \\(n\\) on both sides of the equation and there is no convenient algebraic solution to solving for \\(n\\). Later on we’ll use an R function that accounts for this, but for now we will use the rough approximation. 5.4 Exercises An experiment is conducted to examine the susceptibility of root stocks of a variety of lemon trees to a specific larva. Forty of the plants are subjected to the larvae and examined after a fixed period of time. The response of interest is the logarithm of the number of larvae per gram of of root stock. For these 40 plants, the sample mean is \\(\\bar{x}=11.2\\) and the sample standard deviation is \\(s=1.3\\). Use these data to construct a \\(90\\%\\) confidence interval for \\(\\mu\\), the mean susceptibility of lemon tree root stocks from which the sample was taken. A social worker is interested in estimating the average length of time spent outside of prison for first offenders who later commit a second crime and are sent to prison again. A random sample of \\(n=100\\) prison records in the count courthouse indicates that the average length of prison-free life between first and second offenses is \\(4.2\\) years, with a standard deviation of \\(1.1\\) years. Use this information to construct a \\(95\\%\\) confidence interval for \\(\\mu\\), the average time between first and second offenses for all prisoners on record in the county courthouse. A biologist wishes to estimate the effect of an antibiotic on the growth of a particular bacterium by examining the number of colony forming units (CFUs) per plate of culture when a fixed amount of antibiotic is applied. Previous experimentation with the antibiotic on this type of bacteria indicates that the standard deviation of CFUs is approximately \\(4\\). Using this information, determine the number of observations (i.e. cultures developed) necessary to calculate a \\(99\\%\\) confidence interval with a half-width of \\(1\\). In the R package Lock5Data, the dataset FloridaLakes contains information about the mercury content of fish in 53 Florida lakes. For this question, we’ll be concerned with the average ppm of mercury in fish from those lakes which is encoded in the column AvgMercury. Using the bootstrapping method, calculate a 95% confidence interval for \\(\\mu\\), the average ppm of mercury in fish in all Florida lakes. Using the asymptotic approximations discussed in this chapter, calculate a 95% confidence interval for \\(\\mu\\), the average ppm of mercury in fish in all Florida lakes. Comment on the similarity of these two intervals. In the R package Lock5Data, the dataset Cereal contains nutrition information about a random sample of 30 cereals taken from an on-line nutrition information website (see the help file for the dataset to get the link). For this problem, we’ll consider the column Sugars which records the grams of sugar per cup. Using the bootstrapping method, calculate a 90% confidence interval for \\(\\mu\\), the average grams of sugar per cup of all cereals listed on the website. Using the asymptotic approximations discussed in this chapter, calculate a 90% confidence interval for \\(\\mu\\), the average grams of sugar per cup of all cereals listed on this website. Comment on the similarity of these two intervals. We could easily write a little program (or pay an undergrad) to obtain the nutritional information about all the cereals on the website so the random sampling of 30 cereals is unnecessary. However, a bigger concern is that the website cereals aren’t representative of cereals Americans eat. Why? For example, consider what would happen if we added 30 new cereals that were very nutritious but were never sold. "],
["6-hypothesis-tests-for-the-mean-of-a-population.html", "Chapter 6 Hypothesis Tests for the mean of a population 6.1 Writing Hypotheses 6.2 Type I and Type II Errors 6.3 Exercises", " Chapter 6 Hypothesis Tests for the mean of a population library(dplyr) library(ggplot2) library(tidyr) library(mosaic) Science is done by observing how the world works, making a conjecture (or hypothesis) about the mechanism and then performing experiments to see if real data agrees or disagrees with the proposed hypothesis. Example. Suppose a rancher in Texas (my brother-in-law Bryan) wants to buy some calves from another rancher. This rancher claims that the average weight of his calves is 500 pounds. My brother-in-law likes them and buys 10. A few days later he starts looking at the cows and begins to wonder if the average really is 500 pounds. He weighs his 10 cows and the sample mean is \\(\\bar{x}=475\\) and the sample standard deviation is \\(s=50\\). Below are the data cows &lt;- data.frame( weight = c(553, 466, 451, 421, 523, 517, 451, 510, 392, 466) ) cows %&gt;% summarise( xbar=mean(weight), s=sd(weight) ) ## xbar s ## 1 475 49.99556 There are two possibilities. Either Bryan was just unlucky the random selection of his 10 cows from the heard, or the true average weight within the herd is less than 500. \\[H_{0}:\\;\\mu = 500\\] \\[H_{a}:\\;\\mu &lt; 500\\] For this calculation we’ll assume the weight of a steer is normally distributed \\(N\\left(\\mu,\\sigma\\right)\\), and therefore \\(\\bar{X}\\) is normally distributed \\(N\\left(\\mu,\\frac{\\sigma}{\\sqrt{n}}\\right)\\). If true mean is 500, how likely is it to get a sample mean of 475 (or less)? One way to think about this is that we want a measure of how extreme the event is that we observed, and one way to do that is to calculate how much probability there is for events that are even more extreme. To calculate how far into the tail our observed sample mean \\(\\bar{x}=475\\) is by measuring the area of the distribution that is farther into the tail than the observed value. \\[\\begin{aligned}P\\left(\\bar{X}\\le475\\right) &amp;= P\\left(\\frac{\\bar{X}-\\mu}{\\left(\\frac{s}{\\sqrt{n}}\\right)}\\le\\frac{475-500}{\\left(\\frac{50}{\\sqrt{10}}\\right)}\\right) \\\\ &amp;= P\\left(T_{9}\\le-1.58\\right) \\\\ &amp;= 0.074 \\end{aligned}\\] We see that the observed \\(\\bar{X}\\) is in the tail of the distribution and tends to not support \\(H_{0}\\). P-value is the probability of seeing the observed data or something more extreme given the null hypothesis is true. By “something more extreme”, we mean samples that would be more evidence for the alternative hypothesis. \\[\\mbox{p-value}=P(T_{9}&lt;-1.58)=0.074\\] The above value is the actual value calculated using R # pt(-1.58, df=9) # No Graph mosaic::xpt(-1.58, df=9) # With a graph ## [1] 0.07428219 but using tables typically found in intro statistics books, the most precise thing you would be able to say is \\(0.05 \\le \\mbox{p-value} \\le 0.10\\) So there is a small chance that my brother-in-law just got unlucky with his ten cows. While the data isn’t entirely supportive of \\(H_{0}\\), we don’t have strong enough data to out right reject \\(H_{0}\\). So we will say that we fail to reject \\(H_{0}\\). Notice that we aren’t saying that we accept the null hypothesis, only that there is insufficient evidence to call-out the neighbor as a liar. 6.1 Writing Hypotheses 6.1.1 Null and alternative hypotheses In elementary school most students are taught the scientific method follows the following steps: Ask a question of interest. Construct a hypothesis. Design and conduct an experiment that challenges the hypothesis. Depending on how consistent the data is with the hypothesis: If the observed data is inconsistent with the hypothesis, then we have proven it wrong and we should consider competing hypotheses. If the observed data is consistent with the hypothesis, design a more rigorous experiment to continue testing the hypothesis. Through the iterative process of testing ideas and refining them under the ever growing body of evidence, we continually improve our understanding of how our universe works. The heart of the scientific method is the falsification of hypothesis and statistics is the tool we’ll use to assess the consistency of our data with a hypothesis. Science is done by examining competing ideas for how the world works and throwing evidence at them. Each time a hypothesis is removed, the remaining hypotheses appear to be more credible. This doesn’t mean the remaining hypotheses are correct, only that they are consistent with the available data. In approximately 300 BC, Eratosthenes showed that the world was not flat. (Carl Sagan has an excellent episode of Cosmos on this topic. He did this by measuring the different lengths of shadows of identical sticks in two cities that were 580 miles apart but lay on the same meridian (Alexandria is directly north of Aswan). His proposed alternative was that the Earth was a sphere. While his alternative is not technically true (it is actually an oblate spheroid that bulges at the equator), it was substantially better than the flat world hypothesis. At one point it was believed that plants “ate” the soil and turned it into plant mass. A experiment to test this hypothesis was performed by Johannes Baptista van Helmont in 1648 in which he put exactly 200 pounds of soil in a pot and then grew a willow tree out of it for five years. At the end of the experiment, the pot contained 199.875 pounds of soil and 168 pounds of willow tree. He correctly concluded that the plant matter was not substantially taken from the soil but incorrectly jumped to the conclusion that the mass must of have come from the water that was used to irrigate the willow. It is helpful to our understanding to label the different hypothesis, both the ones being tested and the different alternatives. We’ll label the hypothesis being tested as \\(H_{0}\\) which we often refer to as the “null hypothesis.” The alternative hypothesis, which we’ll denote \\(H_{a}\\), should be the opposite of the null hypothesis. Had Eratosthenes known about modern scientific methods, he would have correctly considered \\(H_{0}\\): the world is flat verses \\(H_{a}\\): the world is not flat and not incorrectly concluded that the world is a sphere. Amusingly Eratosthenes’ data wasn’t inconsistent with the hypothesis that the world was shaped like a donut, but he thought the sphere to be more likely. Likewise Helmont should have considered the hypotheses \\(H_{0}\\): plants only consume soil versus the alternative \\(H_{a}\\): plants consume something besides soil. In both of cases, the observed data was compared to what would have been expected if the null hypothesis was true. If the null was true Eratosthenes would have seen the same length shadow in both cities and Helmont would have seen 168 pounds of willow tree and \\(200-168=32\\) pounds of soil remaining. 6.1.2 Error Unfortunately the world is not a simple place and experiments rarely can isolate exactly the hypothesis being tested. We can repeat an experiment and get slightly different results each time due to variation in weather, temperature, or diligence of the researcher. If we are testing the effectiveness of a new drug to treat a particular disease, we don’t trust the results of a single patient, instead we wish to examine many patients (some that receive the new drug and some the receive the old) to average out the noise between the patients. The questions about how many patients do we need to have and how large of a difference between the treatments is large enough to conclude the new drug is better are the heart of modern statistics. Suppose we consider the population of all US men aged 40-60 with high blood pressure (there might be about 20 million people in this population). We want to know if exercise and ACE inhibitors lower systolic blood pressure better than exercise alone for these people. We’ll consider the null hypothesis that exercise is equivalent to exercise and ACE inhibitors versus exercise is different than exercise and ACE inhibitors. If we could take every single member of the population and expose them to exercise or exercise with ACE inhibitors, we would know for certain how the population reacts to the different treatments. Unfortunately this is too expensive and ethically dubious. Instead of testing the entire population we’ll take a sample of \\(n\\) men from the population and treat half of them with exercise alone and half of them with exercise and ACE inhibitors. What might our data look like if there is a difference between the two treatments at different samples sizes compared to if there is no difference? At small sample sizes it is difficult to distinguish the effect of the treatment when it is masked by individual variation. At high sample sizes, the individual variation is smoothed out and the difference between the treatments can be readily seen. Comparing possible data assuming there is a difference between treatments versus no difference. In the top row of graphs, there is a difference between the Exercise and the Exercise + Inhibitor treatments. However, at small sample sizes, we can’t tell if the observed difference is due to the difference in treatment or just random variation in the data. In the second row, there is no difference between the treatments. When the sample size is large it is easy to see if the treatments differ in their effect on systolic blood pressure, but at medium or small sample sizes, the question is much harder. It is important to recognize that the core of the problem is still “is the observed data consistent with the null hypothesis?” but we now have to consider an addition variability term that is unrelated to the research hypothesis of interest. In the above example, the small sample data is consistent with the null hypothesis even when the null hypothesis is false! Perhaps the hardest part about conducting a hypothesis test is figuring out what the null and alternative hypothesis should be. The null hypothesis is a statement about a population parameter. \\[H_{0}:\\mbox{ population parameter = hypothesized value}\\] and the alternative will be one of \\[\\begin{aligned} H_{a}: \\textrm{population parameter } &amp;&lt; \\textrm{ hypothesized value} \\\\ H_{a}: \\textrm{population parameter } &amp;&gt; \\textrm{ hypothesized value} \\\\ H_{a}: \\textrm{population parameter } &amp;\\ne \\textrm{ hypothesized value} \\end{aligned}\\] The hard part is figuring which of the possible alternatives we should examine. The alternative hypothesis is what the researcher believes is true. By showing that the complement of \\(H_{a}\\) (that is \\(H_{0}\\)) can not be true, we support the alternative which we believe to be true. \\(H_{0}\\) is often a statement of no effect, or no difference between the claimed and observed. Example A light bulb company advertises that their bulbs last for 1000 hours. Consumers will be unhappy if the bulbs last less time, but will not mind if the bulbs last longer. Therefore Consumer Reports might perform a test and would consider the hypotheses \\[H_{0}:\\;\\mu = 1000\\] \\[H_{a}:\\;\\mu &lt; 1000\\] Suppose we perform an experiment with \\(n=20\\) lightbulbs and observe \\(\\bar{x}=980\\) and \\(s=64\\) hours and therefore our test statistic is \\[ t_{19} = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}} = \\frac{980-1000}{64/\\sqrt{20}} = -1.40\\] Then the p-value would be # pt(-1.4, df=19) # No Graph mosaic::xpt(-1.4, df=19 ) # With a Graph ## [1] 0.08881538 and we calculate p-value \\(=P\\left(T_{19}&lt;-1.4\\right)=0.0888\\) Example A computer company is buying resistors from another company. The resistors are supposed to have a resistance of \\(2\\) Ohms and too much or too little resistance is bad. Here we would be testing \\[\\begin{aligned} H_{0}:\\mbox{ }\\mu &amp;= 2 \\\\ H_{a}:\\mbox{ }\\mu &amp;\\ne 2 \\end{aligned}\\] Suppose we perform a test of a random sample of resistors and obtain a test statistics of \\(t_{9}=1.8\\). Because the p-value is “the probability of your data or something more extreme” and in this case more extreme implies extreme values in both tails then mosaic::xpt( c(-1.8, 1.8), df=9) ## [1] 0.05269534 0.94730466 and we calculate \\[\\textrm{p-value} = P\\left(\\left|T_{9}\\right|&gt;1.8\\right)=2P\\left(T_{9}&lt;-1.8\\right)=2\\left(0.0527\\right)=0.105\\] using the R commands 2 * pt(-1.8, df=9) ## [1] 0.1053907 6.1.3 Why should hypotheses use \\(\\mu\\) and not \\(\\bar{x}\\)? There is no need to make a statistical test of the form \\[\\begin{aligned} H_{0}:\\;\\bar{x} &amp;= 3 \\\\ H_{a}:\\;\\bar{x} &amp;\\ne 3 \\end{aligned}\\] because we know the value of \\(\\bar{x}\\); we calculated the value there is no uncertainty to what it is. However I want to use the sample mean \\(\\bar{x}\\) as an estimate of the population mean \\(\\mu\\) and because I don’t know what \\(\\mu\\) is but know that it should be somewhere near \\(\\bar{x}\\), my hypothesis test is a question about \\(\\mu\\) and if it is near the value stated in the null hypothesis. Hypotheses are always statements about population parameters such as \\(\\mu\\) or \\(\\sigma\\) and never about sample statistic values such as \\(\\bar{x}\\) or \\(s\\). Examples A potato chip manufacturer advertises that it sells 16 ounces of chips per bag. A consumer advocacy group wants to test this claim. They take a sample of \\(n=18\\) bags and carefully weights the contents of each bag and calculate a sample mean \\(\\bar{x}=15.8\\) oz and a sample standard deviation of \\(s=0.2\\). State an appropriate null and alternative hypothesis. \\[\\begin{aligned} H_{0}:\\mu &amp;= 16\\mbox{ oz } \\\\ H_{a}:\\mu &amp;&lt; 16\\mbox{ oz } \\end{aligned}\\] Calculate an appropriate test statistic given the sample data. \\[t=\\frac{\\bar{x}-\\mu_{0}}{\\frac{s}{\\sqrt{n}}}=\\frac{15.8-16}{\\frac{.2}{\\sqrt{18}}}=-4.24\\] Calculate the p-value. \\[\\mbox{p-value }=P(T_{17}&lt;-4.24)=0.000276\\] Do you reject or fail to reject the null hypothesis at the \\(\\alpha=0.05\\) level? Because the p-value is less than \\(\\alpha=0.05\\) we will reject the null hypothesis. State your conclusion in terms of the problem. There is statistically significant evidence to conclude that the mean weight of chips is less than 16 oz. A pharmaceutical company has developed an improved pain reliever and believes that it acts faster than the leading brand. It is well known that the leading brand takes \\(25\\) minutes to act. They perform an experiment on \\(16\\) people with pain and record the time until the patient notices pain relief. The sample mean is \\(\\bar{x}=23\\) minutes, and the sample standard deviation was \\(s=10\\) minutes. State an appropriate null and alternative hypothesis. \\[\\begin{aligned} H_{0}:\\mu &amp;= 25\\mbox{ minutes } \\\\ H_{a}:\\mu &amp;&lt; 25\\mbox{ minutes } \\end{aligned}\\] Calculate an appropriate test statistic given the sample data. \\[t_{15}=\\frac{\\bar{x}-\\mu_{0}}{\\frac{s}{\\sqrt{n}}}=\\frac{23-25}{\\frac{10}{\\sqrt{16}}}=-0.8\\] Calculate the p-value. \\[\\mbox{p-value }=P(T_{15}&lt;-0.8)=0.218\\] Do you reject or fail to reject the null hypothesis at the \\(\\alpha=.10\\) level? Since the p-value is larger than my \\(\\alpha\\)-level, I will fail to reject the null hypothesis. State your conclusion in terms of the problem. These data do not provide statistically significant evidence to conclude that this new pain reliever acts faster than the leading brand. Consider the case of SAT test preparation course. They claim that their students perform better than the national average of 1019. We wish to perform a test to discover whether or not that is true. \\[\\begin{aligned} H_{0}:\\,\\mu &amp;= 1019 \\\\ H_{a}:\\,\\mu &amp;&gt; 1019 \\end{aligned}\\] They take a sample of size \\(n=10\\) and the sample mean is \\(\\bar{x}=1020\\), with a sample standard deviation \\(s=50\\). The test statistic is \\[t_{9}=\\frac{\\bar{x}-\\mu_{0}}{\\frac{s}{\\sqrt{n}}}=\\frac{1}{\\frac{50}{\\sqrt{10}}}=.06\\] So the p-value is \\(\\mbox{p-value }=P(T_{9}&gt;.06)\\approx0.5\\) and we fail to reject the null hypothesis. However, what if they had performed this experiment with \\(n=20000\\) students and gotten the same results? \\[t_{19999}=\\frac{\\bar{x}-\\mu_{0}}{\\frac{s}{\\sqrt{n}}}=\\frac{1}{\\frac{50}{\\sqrt{20000}}}=2.83\\] and thus \\(\\mbox{p-value }=P(T_{19999}&gt;2.83)=0.0023\\) At \\(\\alpha=.05\\), we will reject the null hypothesis and conclude that there is statistically significant evidence that the students who take the course perform better than the national average. So what just happened and what does “statistically significant” mean? It appears that there is very slight difference between the students who take the course versus those that don’t. With a small sample size we can not detect that difference, but by taking a large sample size, I can detect the difference of even 1 SAT point. So here I would say that there is a statistical difference between the students who take the course versus those that don’t because given such a large sample, we are very unlikely to see a sample mean of \\(\\bar{x}=1020\\) if the true mean is \\(\\mu=1019\\). So statistically significant really means “unlikely to occur by random chance”. But is there a practical difference in 1 SAT point? Not really. Since SAT scores are measured in multiple of 5 (you can score 1015, or 1020, but not 1019), there isn’t any practical value of raising a students score by 1 point. By taking a sample so large, I have been able to detect a completely worthless difference. Thus we have an example of a statistically significant difference, but it is not a practical difference. 6.1.4 Calculating p-values Students often get confused by looking up probabilities in tables and don’t know which tail of the distribution supports the alternative hypothesis. This is further exacerbated by tables sometimes giving area to the left, sometimes area to the right, and R only giving area to the left. In general, your best approach to calculating p-values correctly is to draw the picture of the distribution of the test statistic (usually a t-distribution) and decide which tail(s) supports the alternative and figuring out the area farther out in the tail(s) than your test statistic. However, since some students need a more algorithmic set of instructions, the following will work: If your alternative has a \\(\\ne\\) sign Look up the value of your test statistic in whatever table you are going to use and get some probability… which I’ll call \\(p^{*}\\). Is \\(p^{*} &gt; 0.5\\)? If so, you just looked up the area in the wrong tail. To fix your error, subtract from one… that is \\(p^{*} \\leftarrow 1-p^{*}\\) Because this is a two sided test, multiply \\(p^{*}\\) by two and that is your p-value. \\(\\textrm{p-value}=2\\left(p^{*}\\right)\\) A p-value is a probability and therefore must be in the range \\([0,1]\\). If what you’ve calculated is outside that range, you’ve made a mistake. If your alternative is \\(&lt;\\) (or \\(&gt;\\)) then the p-value is the area to the left (to the right for the greater than case) of your test statistic. Look up the value of your test statistic in whatever table you are using and get the probability… which again I’ll call \\(p^{*}\\) If \\(p^{*} &gt; 0.5\\), you have most likely screwed up and looked up the area for the wrong tail. Be careful here, because if your alternative is “greater than” and your test statistic is negative, then the p-value really is greater than \\(0.5\\). This situation is rare and 9 times out of 10, the student has just used the table incorrectly. Most of the time you’ll subtract from one \\(p^{*}=1-p^{*}\\). After possibly adjusting for looking up the wrong tail, your p-value is \\(p^{*}\\) with no multiplication necessary. 6.1.5 Calculating p-values vs cutoff values We have been calculating p-values and then comparing those values to the desired alpha level. It is possible, however, to use the alpha level to back-calculate a cutoff level for the test statistic, or even original sample mean. Often these cutoff values are referred to as critical values. Neither approach is wrong, but is generally a matter of preference, although knowing both techniques can be useful. Example. We return to the pharmaceutical company that has developed a new pain reliever. Recall null and alternative hypothesis was \\[\\begin{aligned} H_{0}:\\mu &amp;= 25\\mbox{ minutes } \\\\ H_{a}:\\mu &amp;&lt; 25\\mbox{ minutes } \\end{aligned}\\] and we had observed a test statistic \\[t=\\frac{\\bar{x}-\\mu_{0}}{\\frac{s}{\\sqrt{n}}}=\\frac{23-25}{\\frac{10}{\\sqrt{16}}}=-0.8\\] with \\(15\\) degrees of freedom. Using an \\(\\alpha=0.10\\) level of significance, if this test statistic is smaller than the \\(0.10\\)th quantile of a \\(t\\)-distribution with \\(15\\) degrees of freedom, then we will reject the null hypothesis. This cutoff value is \\(t_{crit}=-1.341\\) and can be using either R or the t-table. Because the observed test statistic is less extreme than the cutoff value, we failed to reject the null hypothesis. We can push this idea even farther and calculate a critical value on the original scale of \\(\\bar{x}\\) by solving \\[\\begin{aligned} t_{crit} &amp;= \\frac{\\bar{x}_{crit}-\\mu_{0}}{\\left( \\frac{s}{\\sqrt{n}} \\right)} \\\\ \\\\ -1.341 &amp;= \\frac{\\bar{x}_{crit}-25}{\\left( \\frac{10}{\\sqrt{16}} \\right) } \\\\ -1.341\\left(\\frac{10}{\\sqrt{16}}\\right)+25 &amp;= \\bar{x}_{crit} \\\\ 21.65 &amp;= \\bar{x}_{crit} \\end{aligned}\\] So if we observe a sample mean \\(\\bar{x}&lt;21.65\\) then we would reject the null hypothesis. Here we actually observed \\(\\bar{x}=23\\) so this comparison still fails to reject the null hypothesis and concludes there is insufficient evidence to reject that the new pain reliever has the same time till relief as the old medicine. In general, I prefer to calculate and report p-values because they already account for any ambiguity in if we are dealing with a 1 sided or 2 sided test and how many degrees of freedom there are. 6.1.6 t-tests in R While it is possible to do t-tests by hand, most people will use a software package to perform these calculations. Here we will use the R function t.test(). This function expects a vector of data (so that it can calculate \\(\\bar{x}\\) and \\(s\\)) and a hypothesized value of \\(\\mu\\). Example. Suppose we have data regarding fuel economy of \\(5\\) vehicles of the same make and model and we wish to test if the observed fuel economy is consistent with the advertised \\(31\\) mpg at highway speeds. Assuming the fuel economy varies normally amongst cars of the same make and model, we test \\[\\begin{aligned} H_{0}:\\,\\mu &amp;= 31 \\\\ H_{a}:\\,\\mu &amp;\\ne 31 \\end{aligned}\\] and calculate cars &lt;- data.frame(mpg = c(31.8, 32.1, 32.5, 30.9, 31.3)) cars %&gt;% summarise(mean(mpg), sd(mpg)) ## mean(mpg) sd(mpg) ## 1 31.72 0.6340347 The test statistic is: \\[t=\\frac{\\bar{x}-\\mu_{0}}{s/\\sqrt{n}}=\\frac{31.72-31}{\\left(\\frac{0.634}{\\sqrt{5}}\\right)}=2.54\\] The p-value is \\[\\textrm{p-value}=2\\cdot P\\left(T_{4}&gt;2.54\\right)=0.064\\] and a \\(95\\%\\) confidence interval is \\[\\begin{aligned} \\bar{x} &amp;\\pm t_{n-1}^{1-\\alpha/2}\\left(\\frac{s}{\\sqrt{n}}\\right) \\\\ 31.72 &amp;\\pm 2.776445\\left(\\frac{0.63403}{\\sqrt{5}}\\right) \\\\ 31.72 &amp;\\pm 0.7872 \\\\ [30.93,\\; &amp; 32.51] \\end{aligned}\\] t.test( cars$mpg, mu=31, alternative=&#39;two.sided&#39; ) ## ## One Sample t-test ## ## data: cars$mpg ## t = 2.5392, df = 4, p-value = 0.06403 ## alternative hypothesis: true mean is not equal to 31 ## 95 percent confidence interval: ## 30.93274 32.50726 ## sample estimates: ## mean of x ## 31.72 The t.test() function supports testing one-sided alternatives and more information can be found in the R help system using help(t.test). 6.2 Type I and Type II Errors We can think of the p-value as measuring how much evidence we have for the null hypothesis. If the p-value is small, the evidence for the null hypothesis is small. Conversely if the p-value is large, then the data is supporting the null hypothesis. There is an important philosophical debate about how much evidence do we need in order to reject the null hypothesis. My brother-in-law would have to have extremely strong evidence before he stated the other rancher was wrong. Likewise, researchers needed solid evidence before concluding that Newton’s Laws of Motion were incorrect. Since the p-value is a measure of support for the null hypothesis, if the p-value drops below a specified threshold (call it \\(\\alpha\\)), I will chose to reject the null hypothesis. Different scientific disciplines have different levels of rigor. Therefore they set commonly used \\(\\alpha\\) levels differently. For example physicists demand a high degree of accuracy and consistency, thus might use \\(\\alpha=0.01\\), while ecologists deal with very messy data and might use an \\(\\alpha=0.10\\). The most commonly used \\(\\alpha\\)-level is \\(\\alpha=0.05\\), which is traditional due to an off-hand comment by R.A. Fisher. There is nothing that fundamentally forces us to use \\(\\alpha=0.05\\) other than tradition. However, when sociologists do experiments presenting subjects with unlikely events, it is usually when the events have a probability around \\(0.05\\) that the subjects begin to suspect they are being duped. People who demand rigor might want to set \\(\\alpha\\) as low as possible, but there is a trade off. Consider the following possibilities, where the “True State of Nature” is along the top, and the decision is along the side. \\(H_0\\) True \\(H_0\\) False Fail to reject \\(H_0\\) :) Type II Error Reject \\(H_0\\) Type II Error :) There are two ways to make a mistake. The type I error is to reject \\(H_{0}\\) when it is true. This error is controlled by \\(\\alpha\\). We can think of \\(\\alpha\\) as the probability of rejecting \\(H_{0}\\) when we shouldn’t. However there is a trade off. If \\(\\alpha\\) is very small then we will fail to reject \\(H_{0}\\) in cases where \\(H_{0}\\) is not true. This is called a type II error and we will define \\(\\beta\\) as the probability of failing to reject \\(H_{0}\\) when it is false. This trade off between type I and type II errors can be seen by examining our legal system. A person is presumed innocent until proven guilty. So the hypothesis being tested in the court of law are \\[\\begin{aligned} H_{0}: &amp; \\textrm{ defendent is innocent} \\\\ H_{a}: &amp; \\textrm{ defendent is guilty} \\end{aligned}\\] Our legal system theoretically operates under the rule that it is better to let 10 guilty people go free, than wrongly convict 1 innocent. In other words, it is worse to make a type I mistake (concluding guilty when innocent), than to make a type II mistake (concluding not guilty when guilty). Critically, when a jury finds a person “not guilty” they are not saying that defense team has proven that the defendant is innocent, but rather that the prosecution has not proven the defendant guilty. This same idea manifests itself in science with the \\(\\alpha\\)-level. Typically we decide that it is better to make a type II mistake. An experiment that results in a large p-value does not prove that \\(H_{0}\\) is true, but that there is insufficient evidence to conclude \\(H_{a}\\). If we still suspect that \\(H_{a}\\) is true, then we must repeat the experiment with a larger samples size. A larger sample size makes it possible to detect smaller differences. 6.2.1 Power and Sample Size Selection Just as we calculated the necessary sample size to achieve a confidence interval of a specified width, we are also often interested in calculating the necessary sample size to to find a significant difference from the hypothesized mean \\(\\mu_{0}\\). Just as in the confidence interval case where we had to specify the half-width \\(E\\) and some estimate of the population standard deviation \\(\\hat{\\sigma}\\), we now must specify a difference we want to be able to detect \\(\\delta\\) and an estimate of the population standard deviation \\(\\hat{\\sigma}\\). Example. Suppose that I work in Quality Control for a company that manufactures a type of rope. This rope is supposed to have a mean breaking strength of \\(5000\\) pounds and long experience with the process suggests that the standard deviation is approximately \\(s=50\\). As with many manufacturing processes, sometimes the machines that create the rope get out of calibration. So each morning we take a random sample of \\(n=7\\) pieces of rope and using \\(\\alpha=0.05\\), test the hypothesis \\[\\begin{aligned} H_{0}:\\;\\mu &amp;= 5000 \\\\ H_{a}:\\;\\mu &amp;&lt; 5000 \\end{aligned}\\] Notice that I will reject the null hypothesis if \\(\\bar{x}\\) is less than some cut-off value (which we denote \\(\\bar{x}_{crit}\\)), which we calculate by first recognizing that the critical t-value is \\[t_{crit}=t_{n-1}^{\\alpha}=-1.943\\] and then solving the following equation for \\(\\bar{x}_{crit}\\) \\[\\begin{aligned} t_{crit} &amp;= \\frac{\\bar{x}_{crit}-\\mu_{0}}{\\frac{s}{\\sqrt{n}}} \\\\ t_{crit}\\left(\\frac{s}{\\sqrt{n}}\\right)+\\mu_{0} &amp;= \\bar{x}_{crit} \\\\ -1.943\\left(\\frac{50}{\\sqrt{7}}\\right)+5000 &amp;= \\bar{x}_{crit} \\\\ 4963 &amp;= \\bar{x}_{crit} \\end{aligned}\\] There is a trade off between the Type I and Type II errors. By making a Type I error, I will reject the null hypothesis when the null hypothesis is true. Here I would stop manufacturing for the day while recalibrating the machine. Clearly a Type I error is not good. The probability of making a Type I error is denoted \\(\\alpha\\). A type II error occurs when I fail to reject the null hypothesis when the alternative is true. This would mean that we would be selling ropes that have a breaking point less than the advertised amount. This opens the company up to a lawsuit. We denote the probability of making a Type II error is denoted as \\(\\beta\\) and define Power \\(=1-\\beta\\). But consider that I don’t want to be shutting down the plant when the breaking point is just a few pounds from the true mean. The head of engineering tells me that if the average breaking point is more than \\(50\\) pounds less than \\(5000\\), we have a problem, but less than \\(50\\) pounds is acceptable. So I want to be able to detect if the true mean is less than \\(4950\\) pounds. Consider the following where we assume \\(\\mu=4950\\). The the probability of a type II error is \\[\\begin{aligned} \\beta &amp;= P\\left(\\bar{X}&gt;4963.3\\;|\\,\\mu=4950\\right) \\\\ &amp;= P\\left(\\frac{\\bar{X}-4950}{50/\\sqrt{7}}&gt;\\frac{4963.3-4950}{50/\\sqrt{7}}\\right) \\\\ &amp;= P\\left(T_{6}&gt;0.703\\right) \\\\ &amp;= 0.254 \\end{aligned}\\] and therefore my power for detecting a mean breaking strength less than or equal to 4950 is \\(1-\\beta=0.7457\\) which is very close to what any statistical package will calculate for us.The power calculation should done using a t-distribution with non-centrality parameter instead of just shifting the distribution. The difference is slight, but is enough to cause our calculation to be slightly off. This power is rather low and I would prefer to have the power be near \\(0.95\\). We can improve our power by using a larger sample size. We’ll repeat these calculations using \\(n=15\\). Power calculations are relatively tedious to do by hand, but fortunately there are several very good resources for exploring how power and sample size interact. My favorite is a Java Applet web page maintained by Dr. Russ Lenth at http://www.stat.uiowa.edu/~rlenth/Power/. It will provide you a list of analysis to do the calculations for and the user is responsible for knowing that we are doing a one-sample t-test with a one-sided alternative. Alternatively, we can do these calculations in R using the function power.t.test(). Fundamentally there are five values that can be used and all power calculators will allow a user to input four of them and the calculator will calculate the fifth. The difference \\(\\delta\\) from the hypothesized mean \\(\\mu_{0}\\) that we wish to detect. The population standard deviation \\(\\sigma\\). The significance level of the test \\(\\alpha\\). The power of the test \\(1-\\beta\\). The sample size \\(n\\). power.t.test(delta=50, sd=50, sig.level=0.05, n=7, type=&quot;one.sample&quot;, alternative=&quot;one.sided&quot;) ## ## One-sample t test power calculation ## ## n = 7 ## delta = 50 ## sd = 50 ## sig.level = 0.05 ## power = 0.7543959 ## alternative = one.sided power.t.test(delta=50, sd=50, sig.level=0.05, power=0.95, type=&quot;one.sample&quot;, alternative=&quot;one.sided&quot;) ## ## One-sample t test power calculation ## ## n = 12.32052 ## delta = 50 ## sd = 50 ## sig.level = 0.05 ## power = 0.95 ## alternative = one.sided The general process for selecting a sample size is to Pick a \\(\\alpha\\)-level. Usually this is easy and people use \\(\\alpha=0.05\\). Come up with an estimate for the standard deviation \\(\\sigma\\). If you don’t have an estimate, then a pilot study should be undertaken to get a rough idea what the variability is. Often this is the only good data that comes out of the first field season in a dissertation. Decide how large of an effect is scientifically interesting. Plug the results of steps 1-3 into a power calculator and see how large a study you need to achieve a power of \\(90\\%\\) or \\(95\\%\\). 6.3 Exercises One way the amount of sewage and industrial pollutants dumped into a body of water affects the health of the water is by reducing the amount of dissolved oxygen available for aquatic life. Over a 2-month period, 8 samples were taken from a river at a location 1 mile downstream from a sewage treatment plant. The amount of dissolved oxygen in the samples was determined and is reported in the following table. 5.1 4.9 5.6 4.2 4.8 4.5 5.3 5.2 Current research suggests that the mean dissolved oxygen level must be at least 5.0 parts per million (ppm) for fish to survive. Do the calculations in parts (b) and (e) by hand. Use R to calculate the sample mean and standard deviation. Using the asymptotic results and the quantities you calculated, by hand calculation create a \\(95\\%\\) two-sided confidence interval for the mean dissolved oxygen level during the 2-month period. What assumption is being made for this calculation to be valid? Calculate a 95% two-sided confidence interval using the bootstrap method. Examine the bootstrap distribution of the sample means, does it appear normal? If so, what does that imply about the assumption you made in the calculation in the previous part? Using the confidence interval calculated in part (b), do the data support the hypothesis that the mean dissolved oxygen level is equal to 5 ppm? Using the quantities you calculated in part (a), by hand perform a 1-sided hypothesis test that the mean oxygen level is less that 5 ppm with a significance level of \\(\\alpha=0.05\\). Use the function t.test in R to repeat the calculations you made in parts (b) and (e). We are interested in investigating how accurate radon detectors sold to homeowners are. We take a randomly selection of \\(n=12\\) detectors and expose them to \\(105\\) pico-curies per liter (pCi/l) of radon. The following values were given by the radon detectors. 91.9 97.8 111.4 122.3 105.4 95 103.8 99.6 96.6 119.3 104.8 101.7 Do all of the following calculations by hand (except for the calculations of the mean and standard deviation). Calculate a \\(90\\%\\) confidence interval using the asymptotic method. State an appropriate null and alternative hypothesis for a two-sided t-test. Why is a two-sided test appropriate here? Calculate an appropriate test statistic. Calculate a p-value. At an \\(\\alpha=0.10\\) level, what is your conclusion. Be sure to state your conclusion in terms of the problem. Use the function t.test() to redo the the hand calculations you did in parts (a), (c), (d). Given data such that \\(X_{i}\\sim N\\left(\\mu,\\sigma^{2}=5^{2}\\right)\\), the following graph shows the distribution of a sample mean of \\(n=8\\) observations under the null hypothesis \\(H_{0}:\\mu=5\\). We are interested in testing the alternative \\(H_{a}:\\mu&gt;5\\) at the \\(\\alpha=0.05\\) level and therefore the cut off point for rejecting the null hypothesis is \\(t_{crit}=1.895\\) and \\(\\bar{x}_{crit}=1.895*5+5=8.35\\). Add the plot of the distribution of the sample mean if \\(\\mu=11\\) and denote which areas represent \\(\\alpha\\), \\(\\beta\\), and the power in the figure below. I expect most people will print out the graph and shade/label everything by hand. Under the same alternative value of \\(\\mu=11\\), find the probability of a Type II error. That is, calculate the value of \\(\\beta=P\\left(\\bar{X}&lt;8.35\\,|\\,\\mu=11\\right)\\). A study is to be undertaken to study the effectiveness of connective tissue massage therapy on the range of motion of the hip joint for elderly clients. Practitioners think that a reasonable standard deviation of the differences (post - pre) would be \\(\\sigma=20\\) degrees. Suppose an increase of 5 degrees in the range would be a clinically significant result. How large of a sample would be necessary if we wanted to control the Type I error rate by \\(\\alpha=0.1\\) and the Type II error rate with \\(\\beta=0.1\\) (therefore the power is \\(1-\\beta=0.90\\))? Use the use the power.t.test() function available in the package pwr to find the necessary sample size. Suppose we were thought that only increases greater than 10 degrees were substantive. How large must our minimum sample size be in this case? Comment on how much larger a sample size must be to detect a difference half as small. "]
]
