<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Statistical Methodology</title>
  <meta name="description" content="Introduction to Statistical Methodology">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Introduction to Statistical Methodology" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="dereksonderegger/STA_570_Book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Statistical Methodology" />
  
  
  

<meta name="author" content="Derek L. Sonderegger">


<meta name="date" content="2017-10-21">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="7-two-sample-hypothesis-tests-and-confidence-intervals.html">
<link rel="next" href="9-analysis-of-variance-anova.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://dereksonderegger.github.io/570/Statistical_Methods_I.pdf" target="blank">PDF version</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html"><i class="fa fa-check"></i><b>1</b> Summary Statistics and Graphing</a><ul>
<li class="chapter" data-level="1.1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#graphical-summaries-of-data"><i class="fa fa-check"></i><b>1.1</b> Graphical summaries of data</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#univariate---categorical"><i class="fa fa-check"></i><b>1.1.1</b> Univariate - Categorical</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#univariate---continuous"><i class="fa fa-check"></i><b>1.1.2</b> Univariate - Continuous</a></li>
<li class="chapter" data-level="1.1.3" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#bivariate---categorical-vs-continuous"><i class="fa fa-check"></i><b>1.1.3</b> Bivariate - Categorical vs Continuous</a></li>
<li class="chapter" data-level="1.1.4" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#bivariate---continuous-vs-continuous"><i class="fa fa-check"></i><b>1.1.4</b> Bivariate - Continuous vs Continuous</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#measures-of-centrality"><i class="fa fa-check"></i><b>1.2</b> Measures of Centrality</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#mean"><i class="fa fa-check"></i><b>1.2.1</b> Mean</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#median"><i class="fa fa-check"></i><b>1.2.2</b> Median</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#mode"><i class="fa fa-check"></i><b>1.2.3</b> Mode</a></li>
<li class="chapter" data-level="1.2.4" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#examples"><i class="fa fa-check"></i><b>1.2.4</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#measures-of-variation"><i class="fa fa-check"></i><b>1.3</b> Measures of Variation</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#range"><i class="fa fa-check"></i><b>1.3.1</b> Range</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#inter-quartile-range"><i class="fa fa-check"></i><b>1.3.2</b> Inter-Quartile Range</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#variance"><i class="fa fa-check"></i><b>1.3.3</b> Variance</a></li>
<li class="chapter" data-level="1.3.4" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#standard-deviation"><i class="fa fa-check"></i><b>1.3.4</b> Standard Deviation</a></li>
<li class="chapter" data-level="1.3.5" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#coefficient-of-variation"><i class="fa fa-check"></i><b>1.3.5</b> Coefficient of Variation</a></li>
<li class="chapter" data-level="1.3.6" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#empirical-rule-of-thumb"><i class="fa fa-check"></i><b>1.3.6</b> Empirical Rule of Thumb</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#exercises"><i class="fa fa-check"></i><b>1.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-probability.html"><a href="2-probability.html"><i class="fa fa-check"></i><b>2</b> Probability</a><ul>
<li class="chapter" data-level="2.1" data-path="2-probability.html"><a href="2-probability.html#introduction-to-set-theory"><i class="fa fa-check"></i><b>2.1</b> Introduction to Set Theory</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-probability.html"><a href="2-probability.html#composition-of-events"><i class="fa fa-check"></i><b>2.1.1</b> Composition of events</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-probability.html"><a href="2-probability.html#probability-rules"><i class="fa fa-check"></i><b>2.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-probability.html"><a href="2-probability.html#simple-rules"><i class="fa fa-check"></i><b>2.2.1</b> Simple Rules</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-probability.html"><a href="2-probability.html#conditional-probability"><i class="fa fa-check"></i><b>2.2.2</b> Conditional Probability</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-probability.html"><a href="2-probability.html#summary-of-probability-rules"><i class="fa fa-check"></i><b>2.2.3</b> Summary of Probability Rules</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-probability.html"><a href="2-probability.html#discrete-random-variables"><i class="fa fa-check"></i><b>2.3</b> Discrete Random Variables</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-probability.html"><a href="2-probability.html#introduction-to-discrete-random-variables"><i class="fa fa-check"></i><b>2.3.1</b> Introduction to Discrete Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-probability.html"><a href="2-probability.html#common-discrete-distributions"><i class="fa fa-check"></i><b>2.4</b> Common Discrete Distributions</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-probability.html"><a href="2-probability.html#binomial-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Binomial Distribution</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-probability.html"><a href="2-probability.html#poisson-distribution"><i class="fa fa-check"></i><b>2.4.2</b> Poisson Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-probability.html"><a href="2-probability.html#continuous-random-variables"><i class="fa fa-check"></i><b>2.5</b> Continuous Random Variables</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-probability.html"><a href="2-probability.html#uniform01-distribution"><i class="fa fa-check"></i><b>2.5.1</b> Uniform(0,1) Distribution</a></li>
<li class="chapter" data-level="2.5.2" data-path="2-probability.html"><a href="2-probability.html#exponential-distribution"><i class="fa fa-check"></i><b>2.5.2</b> Exponential Distribution</a></li>
<li class="chapter" data-level="2.5.3" data-path="2-probability.html"><a href="2-probability.html#normal-distribution"><i class="fa fa-check"></i><b>2.5.3</b> Normal Distribution</a></li>
<li class="chapter" data-level="2.5.4" data-path="2-probability.html"><a href="2-probability.html#standardizing"><i class="fa fa-check"></i><b>2.5.4</b> Standardizing</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="2-probability.html"><a href="2-probability.html#exercises-1"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-confidence-intervals-via-bootstrapping.html"><a href="3-confidence-intervals-via-bootstrapping.html"><i class="fa fa-check"></i><b>3</b> Confidence Intervals via Bootstrapping</a><ul>
<li class="chapter" data-level="3.1" data-path="3-confidence-intervals-via-bootstrapping.html"><a href="3-confidence-intervals-via-bootstrapping.html#theory-of-bootstrapping"><i class="fa fa-check"></i><b>3.1</b> Theory of Bootstrapping</a></li>
<li class="chapter" data-level="3.2" data-path="3-confidence-intervals-via-bootstrapping.html"><a href="3-confidence-intervals-via-bootstrapping.html#quantile-based-confidence-intervals"><i class="fa fa-check"></i><b>3.2</b> Quantile-based Confidence Intervals</a></li>
<li class="chapter" data-level="3.3" data-path="3-confidence-intervals-via-bootstrapping.html"><a href="3-confidence-intervals-via-bootstrapping.html#exercises-2"><i class="fa fa-check"></i><b>3.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html"><i class="fa fa-check"></i><b>4</b> Sampling Distribution of <span class="math inline">\(\bar{X}\)</span></a><ul>
<li class="chapter" data-level="4.1" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#enlightening-example"><i class="fa fa-check"></i><b>4.1</b> Enlightening Example</a></li>
<li class="chapter" data-level="4.2" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#mathematical-details"><i class="fa fa-check"></i><b>4.2</b> Mathematical details</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#probability-rules-for-expectations-and-variances"><i class="fa fa-check"></i><b>4.2.1</b> Probability Rules for Expectations and Variances</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i><b>4.2.2</b> Mean and Variance of the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#distribution-of-barx"><i class="fa fa-check"></i><b>4.3</b> Distribution of <span class="math inline">\(\bar{X}\)</span></a></li>
<li class="chapter" data-level="4.4" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#central-limit-theorem"><i class="fa fa-check"></i><b>4.4</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="4.5" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#exercises-3"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html"><i class="fa fa-check"></i><b>5</b> Confidence Intervals for <span class="math inline">\(\mu\)</span></a><ul>
<li class="chapter" data-level="5.1" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html#asymptotic-result-sigma-known"><i class="fa fa-check"></i><b>5.1</b> Asymptotic result (<span class="math inline">\(\sigma\)</span> known)</a></li>
<li class="chapter" data-level="5.2" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html#asymptotoic-result-sigma-unknown"><i class="fa fa-check"></i><b>5.2</b> Asymptotoic result (<span class="math inline">\(\sigma\)</span> unknown)</a></li>
<li class="chapter" data-level="5.3" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html#sample-size-selection"><i class="fa fa-check"></i><b>5.3</b> Sample Size Selection</a></li>
<li class="chapter" data-level="5.4" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html#exercises-4"><i class="fa fa-check"></i><b>5.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html"><i class="fa fa-check"></i><b>6</b> Hypothesis Tests for the mean of a population</a><ul>
<li class="chapter" data-level="6.1" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#writing-hypotheses"><i class="fa fa-check"></i><b>6.1</b> Writing Hypotheses</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#null-and-alternative-hypotheses"><i class="fa fa-check"></i><b>6.1.1</b> Null and alternative hypotheses</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#error"><i class="fa fa-check"></i><b>6.1.2</b> Error</a></li>
<li class="chapter" data-level="6.1.3" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#why-should-hypotheses-use-mu-and-not-barx"><i class="fa fa-check"></i><b>6.1.3</b> Why should hypotheses use <span class="math inline">\(\mu\)</span> and not <span class="math inline">\(\bar{x}\)</span>?</a></li>
<li class="chapter" data-level="6.1.4" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#calculating-p-values"><i class="fa fa-check"></i><b>6.1.4</b> Calculating p-values</a></li>
<li class="chapter" data-level="6.1.5" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#calculating-p-values-vs-cutoff-values"><i class="fa fa-check"></i><b>6.1.5</b> Calculating p-values vs cutoff values</a></li>
<li class="chapter" data-level="6.1.6" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#t-tests-in-r"><i class="fa fa-check"></i><b>6.1.6</b> t-tests in R</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#type-i-and-type-ii-errors"><i class="fa fa-check"></i><b>6.2</b> Type I and Type II Errors</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#power-and-sample-size-selection"><i class="fa fa-check"></i><b>6.2.1</b> Power and Sample Size Selection</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#exercises-5"><i class="fa fa-check"></i><b>6.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><i class="fa fa-check"></i><b>7</b> Two-Sample Hypothesis Tests and Confidence Intervals</a><ul>
<li class="chapter" data-level="7.1" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#difference-in-means-between-two-groups"><i class="fa fa-check"></i><b>7.1</b> Difference in means between two groups</a><ul>
<li class="chapter" data-level="7.1.1" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#inference-via-resampling"><i class="fa fa-check"></i><b>7.1.1</b> Inference via resampling</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#inference-via-asymptotic-results-unequal-variance-assumption"><i class="fa fa-check"></i><b>7.1.2</b> Inference via asymptotic results (unequal variance assumption)</a></li>
<li class="chapter" data-level="7.1.3" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#inference-via-asymptotic-results-equal-variance-assumption"><i class="fa fa-check"></i><b>7.1.3</b> Inference via asymptotic results (equal variance assumption)</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#difference-in-means-between-two-groups-paired-data"><i class="fa fa-check"></i><b>7.2</b> Difference in means between two groups: Paired Data</a></li>
<li class="chapter" data-level="7.3" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#exercises-6"><i class="fa fa-check"></i><b>7.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html"><i class="fa fa-check"></i><b>8</b> Testing Model Assumptions</a><ul>
<li class="chapter" data-level="8.1" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#testing-normality"><i class="fa fa-check"></i><b>8.1</b> Testing Normality</a><ul>
<li class="chapter" data-level="8.1.1" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#visual-inspection---qqplots"><i class="fa fa-check"></i><b>8.1.1</b> Visual Inspection - QQplots</a></li>
<li class="chapter" data-level="8.1.2" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#tests-for-normality"><i class="fa fa-check"></i><b>8.1.2</b> Tests for Normality</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#testing-equal-variance"><i class="fa fa-check"></i><b>8.2</b> Testing Equal Variance</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#visual-inspection"><i class="fa fa-check"></i><b>8.2.1</b> Visual Inspection</a></li>
<li class="chapter" data-level="8.2.2" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#tests-for-equal-variance"><i class="fa fa-check"></i><b>8.2.2</b> Tests for Equal Variance</a></li>
<li class="chapter" data-level="8.2.3" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#symmetry-of-the-f-distribution"><i class="fa fa-check"></i><b>8.2.3</b> Symmetry of the F-distribution</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#power-of-the-f-test"><i class="fa fa-check"></i><b>8.3</b> Power of the F-test</a></li>
<li class="chapter" data-level="8.4" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#theoretical-distribution-vs-bootstrap"><i class="fa fa-check"></i><b>8.4</b> Theoretical distribution vs bootstrap</a></li>
<li class="chapter" data-level="8.5" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#exercises-7"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html"><i class="fa fa-check"></i><b>9</b> Analysis of Variance (ANOVA)</a><ul>
<li class="chapter" data-level="9.1" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#model"><i class="fa fa-check"></i><b>9.1</b> Model</a></li>
<li class="chapter" data-level="9.2" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#theory"><i class="fa fa-check"></i><b>9.2</b> Theory</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#anova-table"><i class="fa fa-check"></i><b>9.2.1</b> Anova Table</a></li>
<li class="chapter" data-level="9.2.2" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#anova-using-simple-vs-complex-models."><i class="fa fa-check"></i><b>9.2.2</b> ANOVA using Simple vs Complex models.</a></li>
<li class="chapter" data-level="9.2.3" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#parameter-estimates-and-confidence-intervals"><i class="fa fa-check"></i><b>9.2.3</b> Parameter Estimates and Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#anova-in-r"><i class="fa fa-check"></i><b>9.3</b> Anova in R</a></li>
<li class="chapter" data-level="9.4" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#multiple-comparisons"><i class="fa fa-check"></i><b>9.4</b> Multiple comparisons</a></li>
<li class="chapter" data-level="9.5" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#different-model-representations"><i class="fa fa-check"></i><b>9.5</b> Different Model Representations</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#theory-1"><i class="fa fa-check"></i><b>9.5.1</b> Theory</a></li>
<li class="chapter" data-level="9.5.2" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#model-representations-in-r"><i class="fa fa-check"></i><b>9.5.2</b> Model Representations in R</a></li>
<li class="chapter" data-level="9.5.3" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#implications-on-the-anova-table"><i class="fa fa-check"></i><b>9.5.3</b> Implications on the ANOVA table</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#exercises-8"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-regression.html"><a href="10-regression.html"><i class="fa fa-check"></i><b>10</b> Regression</a><ul>
<li class="chapter" data-level="10.1" data-path="10-regression.html"><a href="10-regression.html#pearsons-correlation-coefficient"><i class="fa fa-check"></i><b>10.1</b> Pearson’s Correlation Coefficient</a></li>
<li class="chapter" data-level="10.2" data-path="10-regression.html"><a href="10-regression.html#model-theory"><i class="fa fa-check"></i><b>10.2</b> Model Theory</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10-regression.html"><a href="10-regression.html#anova-interpretation"><i class="fa fa-check"></i><b>10.2.1</b> Anova Interpretation</a></li>
<li class="chapter" data-level="10.2.2" data-path="10-regression.html"><a href="10-regression.html#confidence-intervals-vs-prediction-intervals"><i class="fa fa-check"></i><b>10.2.2</b> Confidence Intervals vs Prediction Intervals</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10-regression.html"><a href="10-regression.html#extrapolation"><i class="fa fa-check"></i><b>10.3</b> Extrapolation</a></li>
<li class="chapter" data-level="10.4" data-path="10-regression.html"><a href="10-regression.html#checking-model-assumptions"><i class="fa fa-check"></i><b>10.4</b> Checking Model Assumptions</a></li>
<li class="chapter" data-level="10.5" data-path="10-regression.html"><a href="10-regression.html#common-problems"><i class="fa fa-check"></i><b>10.5</b> Common Problems</a><ul>
<li class="chapter" data-level="10.5.1" data-path="10-regression.html"><a href="10-regression.html#influential-points"><i class="fa fa-check"></i><b>10.5.1</b> Influential Points</a></li>
<li class="chapter" data-level="10.5.2" data-path="10-regression.html"><a href="10-regression.html#transformations"><i class="fa fa-check"></i><b>10.5.2</b> Transformations</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="10-regression.html"><a href="10-regression.html#exercises-9"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html"><i class="fa fa-check"></i><b>11</b> Resampling Linear Models</a><ul>
<li class="chapter" data-level="11.1" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#using-lm-for-many-analyses"><i class="fa fa-check"></i><b>11.1</b> Using <code>lm()</code> for many analyses</a><ul>
<li class="chapter" data-level="11.1.1" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#one-sample-t-tests"><i class="fa fa-check"></i><b>11.1.1</b> One-sample t-tests</a></li>
<li class="chapter" data-level="11.1.2" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#two-sample-t-tests"><i class="fa fa-check"></i><b>11.1.2</b> Two-sample t-tests</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#creating-simulated-data"><i class="fa fa-check"></i><b>11.2</b> Creating Simulated Data</a><ul>
<li class="chapter" data-level="11.2.1" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#observational-studies-vs-designed-experiments"><i class="fa fa-check"></i><b>11.2.1</b> Observational Studies vs Designed Experiments</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#confidence-interval-types"><i class="fa fa-check"></i><b>11.3</b> Confidence Interval Types</a><ul>
<li class="chapter" data-level="11.3.1" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#normal-intervals"><i class="fa fa-check"></i><b>11.3.1</b> Normal intervals</a></li>
<li class="chapter" data-level="11.3.2" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#percentile-intervals"><i class="fa fa-check"></i><b>11.3.2</b> Percentile intervals</a></li>
<li class="chapter" data-level="11.3.3" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#basic-intervals"><i class="fa fa-check"></i><b>11.3.3</b> Basic intervals</a></li>
<li class="chapter" data-level="11.3.4" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#towards-bias-corrected-and-accelerated-intervals-bca"><i class="fa fa-check"></i><b>11.3.4</b> Towards bias-corrected and accelerated intervals (BCa)</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#bootstrap-confidence-intervals-in-r"><i class="fa fa-check"></i><b>11.4</b> Bootstrap Confidence Intervals in R</a><ul>
<li class="chapter" data-level="11.4.1" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#using-carboot-function"><i class="fa fa-check"></i><b>11.4.1</b> Using <code>car::Boot()</code> function</a></li>
<li class="chapter" data-level="11.4.2" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#using-the-boot-package"><i class="fa fa-check"></i><b>11.4.2</b> Using the <code>boot</code> package</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11-resampling-linear-models.html"><a href="11-resampling-linear-models.html#exercises-10"><i class="fa fa-check"></i><b>11.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-contingency-tables.html"><a href="12-contingency-tables.html"><i class="fa fa-check"></i><b>12</b> Contingency Tables</a><ul>
<li class="chapter" data-level="12.1" data-path="12-contingency-tables.html"><a href="12-contingency-tables.html#expected-counts"><i class="fa fa-check"></i><b>12.1</b> Expected Counts</a></li>
<li class="chapter" data-level="12.2" data-path="12-contingency-tables.html"><a href="12-contingency-tables.html#hypothesis-testing"><i class="fa fa-check"></i><b>12.2</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="12.3" data-path="12-contingency-tables.html"><a href="12-contingency-tables.html#rxc-tables"><i class="fa fa-check"></i><b>12.3</b> RxC tables</a></li>
<li class="chapter" data-level="12.4" data-path="12-contingency-tables.html"><a href="12-contingency-tables.html#exercises-11"><i class="fa fa-check"></i><b>12.4</b> Exercises</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Statistical Methodology</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="testing-model-assumptions" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Testing Model Assumptions</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(dplyr)</code></pre></div>
<p>Performing a t-test requires that the data was drawn from a normal distribution or that the sample size is large enough that the Central Limit Theorem will guarantee that the sample means are approximately normally distributed. However, how do you decide if the data were drawn from a normal distribution, say if your sample size is between 10 and 20? If we are using a model that assumes equal variance between groups, how should we test if that assumption is true?</p>
<div id="testing-normality" class="section level2">
<h2><span class="header-section-number">8.1</span> Testing Normality</h2>
<div id="visual-inspection---qqplots" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Visual Inspection - QQplots</h3>
<p>If we are taking a sample of size <span class="math inline">\(n=10\)</span> from a standard normal distribution, then I should expect that the smallest observation will be negative. Intuitively, you would expect the smallest observation to be near the 10th percentile of the standard normal, and likewise the second smallest should be near the 20th percentile.</p>
<p>This idea needs a little modification because the largest observation cannot be near the 100th percentile (because that is <span class="math inline">\(\infty\)</span>). So we’ll adjust the estimates to still be spaced at (1/n) quantile increments, but starting at the 0.5/n quantile instead of the 1/n quantile. So the smallest observation should be near the 0.05 quantile, the second smallest should be near the 0.15 quantile, and the largest observation should be near the 0.95 quantile. I will refer to these as the theoretical quantiles.</p>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-164-1.png" width="672" /></p>
<p>I can then graph the theoretical quantiles vs my observed values and if they lie on the 1-to-1 line, then my data comes from a standard normal distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">10</span>
data &lt;-<span class="st"> </span><span class="kw">data.frame</span>( <span class="dt">observed    =</span> <span class="kw">sort</span>( <span class="kw">rnorm</span>(n, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>) ),
                    <span class="dt">theoretical =</span> <span class="kw">qnorm</span>( (<span class="dv">1</span>:n -.<span class="dv">5</span>)/n, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span> ) )
<span class="kw">library</span>(ggplot2)
<span class="kw">ggplot</span>(data) +
<span class="st">  </span><span class="kw">geom_point</span>( <span class="kw">aes</span>(<span class="dt">x=</span>theoretical, <span class="dt">y=</span>observed) ) +
<span class="st">  </span><span class="kw">geom_line</span>(  <span class="kw">aes</span>(<span class="dt">x=</span>theoretical, <span class="dt">y=</span>theoretical) ) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">&#39;Theoretical&#39;</span>, <span class="dt">y=</span><span class="st">&#39;Observed&#39;</span>, <span class="dt">title=</span><span class="st">&#39;Q-Q Plot: Observed vs Normal Distribution&#39;</span>)</code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-165-1.png" width="672" /></p>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-166-1.png" width="672" /></p>
<p>If I think my data are normal, but with some mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, we still make the same graph, but the 1-to-1 line will be moved to pass through the 1st and 3rd quartiles. Again, the data points should be near the line. This is common enough that R has built in functions to make this graph:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">10</span>
x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dt">mean=</span><span class="dv">100</span>, <span class="dt">sd=</span><span class="dv">10</span>)
<span class="kw">qqnorm</span>(x)
<span class="kw">qqline</span>(x)</code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-167-1.png" width="672" /></p>
<p>We now will examine a sample of <span class="math inline">\(n=40\)</span> from a bunch of different distributions that are not normal and see what the normal QQ plot looks like. In the following graphs, pay particular attention to the tails. Notice the the t-distribution has significantly heavier tails than the normal distribution and that is reflected in the dots being lower than the line on the left and higher on the right. Likewise the logNormal distribution, which is defined by <span class="math inline">\(\log(X)\sim\)</span> Normal has too light of a tail on the left (because logNormal variables must be greater than 0) and too heavy on the right. The uniform distribution, which is cut off at 0 and 1, has too light of tails in both directions.</p>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-168-1.png" width="576" /></p>
</div>
<div id="tests-for-normality" class="section level3">
<h3><span class="header-section-number">8.1.2</span> Tests for Normality</h3>
<p>It seems logical that there should be some sort of statistical test for if a sample is obviously non-normal. Two common ones are the Shapiro-Wilks test and the Anderson-Darling test. The Shapiro-Wilks test is available in the base installation of R with the function shapiro.test(). The Anderson-Darling test is available in the package <code>nortest</code>. Here we will not focus on the theory of these tests, but instead their use. In both tests the null hypothesis is that the data are normally distributed. <span class="math display">\[\begin{aligned}
H_{0} &amp;:\,  \textrm{data are normally distributed} \\   
H_{a} &amp;:\,  \textrm{data are not normally distributed}  
\end{aligned}\]</span> Therefore a small p-value is evidence against normality.</p>
<p>Often we want to know if our data comes from a normal distribution because our sample size is too small to rely on the Central Limit Theorem to guarantee that the sampling distribution of the sample mean is Normal. So how well do these tests detect non-normality in a small sample size case?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">rlnorm</span>(<span class="dt">n=</span><span class="dv">10</span>, <span class="dt">meanlog=</span><span class="dv">2</span>, <span class="dt">sdlog=</span><span class="dv">2</span>)
<span class="kw">shapiro.test</span>(x)</code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.39539, p-value = 2.207e-07</code></pre>
<p>So the Shapiro-Wilks test detects the non-normality in the extreme case of a logNormal distribution, but what about something closer to normal like the gamma distribution?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">rgamma</span>(<span class="dt">n=</span><span class="dv">10</span>, <span class="dt">shape=</span><span class="dv">5</span>, <span class="dt">rate=</span><span class="dv">1</span>/<span class="dv">5</span>)
<span class="kw">shapiro.test</span>(x)</code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.92703, p-value = 0.4193</code></pre>
<p>Here the Shapiro test fails to detect the sample has non-normality due to the small sample size. Unfortunately, the small sample-size case is exactly when we need a good test. So what do we do?</p>
<p>My advise is to look at the histograms of your data, normal QQ plots, and to use the Shapiro-Wilks test to find extreme non-normality, but recognize that in the small sample case, we have very little power and can only detect extreme departures from normality. If I cannot detect non-normality and my sample size is moderate (15-30), I won’t worry too much since the data isn’t too far from normal and the CLT will help normalize the sample means but for smaller sample sizes, I will use non-parametric methods (such as the bootstrap) that do not make distributional assumptions.</p>
</div>
</div>
<div id="testing-equal-variance" class="section level2">
<h2><span class="header-section-number">8.2</span> Testing Equal Variance</h2>
<div id="visual-inspection" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Visual Inspection</h3>
<p>Often a test procedure assumes equal variances amongst groups or constant variance along a prediction gradient. The most effect way of checking to see if that assumption is met is to visually inspect the data. For the case of t-tests, boxplots are an excellent visual check. If the lengths of the boxes are not substantially different, then the equal variance assumption is acceptable.</p>
<p>Consider an experiment where we measure the speed of reaction to a stimulus. The subjects are told to press a button as soon as they hear a noise. Between 2 and 30 seconds later an extremely loud noise is made. Of primary interest is how inebriation affects the reaction speed. Since we can’t surprise subjects twice, only one measurement per subject is possible and a paired test is not possible. Subjects were randomly assigned to a control or alcohol group</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Alcohol &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">time=</span><span class="kw">c</span>( <span class="fl">0.90</span>, <span class="fl">0.37</span>, <span class="fl">1.63</span>, <span class="fl">0.83</span>, <span class="fl">0.95</span>, <span class="fl">0.78</span>, <span class="fl">0.86</span>, <span class="fl">0.61</span>, <span class="fl">0.38</span>, <span class="fl">1.97</span>,
          <span class="fl">1.46</span>, <span class="fl">1.45</span>, <span class="fl">1.76</span>, <span class="fl">1.44</span>, <span class="fl">1.11</span>, <span class="fl">3.07</span>, <span class="fl">0.98</span>, <span class="fl">1.27</span>, <span class="fl">2.56</span>, <span class="fl">1.32</span> ),
  <span class="dt">trt =</span> <span class="kw">rep</span>(<span class="kw">c</span>(<span class="st">&#39;control&#39;</span>,<span class="st">&#39;alcohol&#39;</span>), <span class="dt">each=</span><span class="dv">10</span>))

<span class="kw">ggplot</span>(Alcohol, <span class="kw">aes</span>(<span class="dt">x=</span>trt, <span class="dt">y=</span>time)) +
<span class="st">  </span><span class="kw">geom_boxplot</span>()</code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-171-1.png" width="672" /></p>
</div>
<div id="tests-for-equal-variance" class="section level3">
<h3><span class="header-section-number">8.2.2</span> Tests for Equal Variance</h3>
<p>Consider having samples drawn from normal distributions <span class="math display">\[X_{ij}=\mu_{i}+\epsilon_{ij}\;\;\;\;\;\;\textrm{where}\;\epsilon_{ij}\sim N\left(0,\,\sigma_{i}^{2}\right)\]</span> where the <span class="math inline">\(i\)</span> subscript denotes which population the observation was drawn from and the <span class="math inline">\(j\)</span> subscript denotes the individual observation and from the <span class="math inline">\(i\)</span>th population we observe <span class="math inline">\(n_i\)</span> samples. In general I might be interested in evaluating if <span class="math inline">\(\sigma_i^2=\sigma_j^2\)</span>.</p>
<p>Let’s consider the simplest case of two populations and consider the null and alternative hypotheses: <span class="math display">\[\begin{aligned}
H_{0} &amp;:\,\sigma_{1}^{2}    =   \sigma_{2}^{2} \\
H_{a} &amp;:\,\sigma_{1}^{2}    \ne \sigma_{2}^{2}
\end{aligned}\]</span> If the null hypothesis is true, then the ratio <span class="math inline">\(s_{1}^{2}/s_{2}^{2}\)</span> should be approximately one. It can be shown that under the null hypothesis, <span class="math display">\[f=\frac{s_{1}^{2}}{s_{2}^{2}}\sim F_{df_{1},df_{2}}\]</span> where <span class="math inline">\(df_{1}\)</span> and <span class="math inline">\(df_{2}\)</span> are the associated degrees of freedom for <span class="math inline">\(s_{1}^{2}\)</span> and <span class="math inline">\(s_{2}^{2}\)</span>. The order of these is traditionally given with the degrees of freedom of the top term first and the degrees of freedom of the bottom term second.</p>
<p>Variables that follow a F distribution must be non-negative and two F distributions are shown below. The F distribution is centered at <span class="math inline">\(E\left(F_{df_{1},df_{2}}\right)=\frac{df_{2}}{df_{2}-2}\approx 1\)</span> for large values of <span class="math inline">\(df_{2}\)</span>. The variance of this distribution goes to 0 as <span class="math inline">\(df_{1}\)</span> and <span class="math inline">\(df_{2}\)</span> get large.</p>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-172-1.png" width="672" /></p>
<p>If the value of my test statistic <span class="math inline">\(f=s_{1}^{2}/s_{2}^{2}\)</span> is too large or too small, then we will reject the null hypothesis. If we preform an F-test with an <span class="math inline">\(\alpha=0.05\)</span> level of significance then we’ll reject <span class="math inline">\(H_{0}\)</span> if <span class="math inline">\(f&lt;F_{0.025,n_{1}-1,n_{2}-1}\)</span> or if <span class="math inline">\(f&gt;F_{0.975,n_{1}-1,n_{2}-1}\)</span>.</p>
<p><strong>Example</strong>. Suppose we have two samples drawn from normally distributed populations. The first has <span class="math inline">\(n_{1}=7\)</span> observations and a sample variance of <span class="math inline">\(s_{1}^{2}=25\)</span> and the second sample has <span class="math inline">\(n_{2}=10\)</span> and <span class="math inline">\(s_{2}^{2}=64\)</span>. Then <span class="math inline">\(f_{6,9}=\frac{25}{64}=0.391\)</span> and we notice this value is is in between the lower and upper cut-off values</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qf</span>( <span class="kw">c</span>(<span class="fl">0.025</span>, .<span class="dv">975</span>), <span class="dv">6</span>, <span class="dv">9</span>)</code></pre></div>
<pre><code>## [1] 0.1810477 4.3197218</code></pre>
<p>so we will fail to reject the null hypothesis. Just for good measure, we can calculate the p-value as <span class="math display">\[\begin{aligned}p-value    
  &amp;=    2\cdot P(F_{n_{1}-1,n_{2}-1}&lt;0.391) \\
    &amp;=  2\cdot P\left(F_{6,9}&lt;0.391\right)
\end{aligned}\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">2</span>*<span class="kw">pf</span>(<span class="fl">0.391</span>, <span class="dv">6</span>, <span class="dv">9</span>)</code></pre></div>
<pre><code>## [1] 0.2654714</code></pre>
<p>We calculate the p-value by finding the area to the left and multiplying by two because my test statistic was less than 1 (the expected value of f if <span class="math inline">\(H_{0}\)</span> is true). If my test statistic was greater than 1, we would have found the area to the right of <span class="math inline">\(f\)</span> and multiplied by two.</p>
</div>
<div id="symmetry-of-the-f-distribution" class="section level3">
<h3><span class="header-section-number">8.2.3</span> Symmetry of the F-distribution</h3>
<p>When testing <span class="math display">\[\begin{aligned}
H_{0} &amp;:        \sigma_{1}^{2}=\sigma_{2}^{2}    \\
H_{a} &amp;:        \sigma_{1}^{2}\ne\sigma_{2}^{2}
\end{aligned}\]</span></p>
<p>The labeling of group <span class="math inline">\(1\)</span> and group <span class="math inline">\(2\)</span> is completely arbitrary and I should view <span class="math inline">\(f=s_{1}^{2}/s_{2}^{2}\)</span> as the same evidence against null as <span class="math inline">\(f^{*}=s_{2}^{2}/s_{1}^{2}\)</span>. Therefore we have <span class="math display">\[
P\left(F_{df_{1},\,df_{2}}&gt;\frac{s_{1}^{2}}{s_{2}^{2}}\right)=P\left(F_{df_{2},\,df_{1}}&lt;\frac{s_{2}^{2}}{s_{1}^{2}}\right)
\]</span> For example, suppose that <span class="math inline">\(n_{1}=5\)</span> and <span class="math inline">\(n_{2}=20\)</span> and <span class="math inline">\(s_{1}^{2}=6\)</span> and <span class="math inline">\(s_{2}^{2}=3\)</span> then <span class="math display">\[P\left(F_{4,\,19}&gt;\frac{6}{3}\right)=P\left(F_{19,\,4}&lt;\frac{3}{6}\right)\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span> -<span class="st"> </span><span class="kw">pf</span>(<span class="dv">6</span>/<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">19</span>)</code></pre></div>
<pre><code>## [1] 0.1354182</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pf</span>(<span class="dv">3</span>/<span class="dv">6</span>, <span class="dv">19</span>, <span class="dv">4</span>)</code></pre></div>
<pre><code>## [1] 0.1354182</code></pre>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-176-1.png" width="672" /></p>
</div>
</div>
<div id="power-of-the-f-test" class="section level2">
<h2><span class="header-section-number">8.3</span> Power of the F-test</h2>
<p>We now turn to the question of how well does this test work? To find out we’ll take samples from normal distributions with different variances and apply our F-test to see how sensitive the test is.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">535</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sigma1 &lt;-<span class="st"> </span><span class="dv">1</span>
sigma2 &lt;-<span class="st"> </span><span class="dv">2</span>
n1 &lt;-<span class="st"> </span><span class="dv">10</span>
n2 &lt;-<span class="st"> </span><span class="dv">10</span>
v1 &lt;-<span class="st"> </span><span class="kw">var</span>(<span class="kw">rnorm</span>(n1, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span>sigma1))
v2 &lt;-<span class="st"> </span><span class="kw">var</span>(<span class="kw">rnorm</span>(n2, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span>sigma2))
f &lt;-<span class="st"> </span>v1/v2
if( f &lt;<span class="st"> </span><span class="dv">1</span> ){
  p.value &lt;-<span class="st"> </span><span class="dv">2</span> *<span class="st">      </span><span class="kw">pf</span>( f, <span class="dt">df1 =</span> n1<span class="dv">-1</span>, <span class="dt">df2 =</span> n2<span class="dv">-1</span> )
}else{
  p.value &lt;-<span class="st"> </span><span class="dv">2</span> *<span class="st"> </span>(<span class="dv">1</span> -<span class="st"> </span><span class="kw">pf</span>( f, <span class="dt">df1 =</span> n1<span class="dv">-1</span>, <span class="dt">df2 =</span> n2<span class="dv">-1</span>))
}
p.value</code></pre></div>
<pre><code>## [1] 0.1142902</code></pre>
<p>So even though the standard deviation in the second sample was twice as large as the first, we were unable to detect it do to the small sample sizes. What happens when we take a larger sample size?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sigma1 &lt;-<span class="st"> </span><span class="dv">1</span>
sigma2 &lt;-<span class="st"> </span><span class="dv">2</span>
n1 &lt;-<span class="st"> </span><span class="dv">30</span>
n2 &lt;-<span class="st"> </span><span class="dv">30</span>
v1 &lt;-<span class="st"> </span><span class="kw">var</span>(<span class="kw">rnorm</span>(n1, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span>sigma1))
v2 &lt;-<span class="st"> </span><span class="kw">var</span>(<span class="kw">rnorm</span>(n2, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span>sigma2))
f &lt;-<span class="st"> </span>v1/v2
if( f &lt;<span class="st"> </span><span class="dv">1</span> ){
  p.value &lt;-<span class="st"> </span><span class="dv">2</span> *<span class="st">      </span><span class="kw">pf</span>( f, <span class="dt">df1 =</span> n1<span class="dv">-1</span>, <span class="dt">df2 =</span> n2<span class="dv">-1</span> )
}else{
  p.value &lt;-<span class="st"> </span><span class="dv">2</span> *<span class="st"> </span>(<span class="dv">1</span> -<span class="st"> </span><span class="kw">pf</span>( f, <span class="dt">df1 =</span> n1<span class="dv">-1</span>, <span class="dt">df2 =</span> n2<span class="dv">-1</span>))
}
p.value</code></pre></div>
<pre><code>## [1] 4.276443e-06</code></pre>
<p>What this tells us is that just like every other statistical test, <em>sample size effects the power of the test</em>. In small sample situations, you cannot rely on a statistical test to tell you if your samples have unequal variance. Instead you need to think about if the assumption is scientifically valid or if you can use a test that does not rely on the equal variance assumption.</p>
</div>
<div id="theoretical-distribution-vs-bootstrap" class="section level2">
<h2><span class="header-section-number">8.4</span> Theoretical distribution vs bootstrap</h2>
<p>Returning to the research example with the alcohol and control group, an F-test for different variances results in a p-value of</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Calculating everything by hand</span>
F &lt;-<span class="st"> </span>Alcohol %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(trt) %&gt;%<span class="st">                </span><span class="co"># for each trt group,</span>
<span class="st">  </span><span class="kw">summarise</span>( <span class="dt">s2 =</span> <span class="kw">var</span>(time)) %&gt;%<span class="st">   </span><span class="co"># calculate variance.</span>
<span class="st">  </span><span class="kw">summarise</span>( <span class="dt">F =</span> s2[<span class="dv">1</span>] /<span class="st"> </span>s2[<span class="dv">2</span>] )   <span class="co"># and then take the ratio</span>
F</code></pre></div>
<pre><code>## # A tibble: 1 x 1
##          F
##      &lt;dbl&gt;
## 1 1.704753</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">obs.F &lt;-<span class="st"> </span><span class="kw">as.numeric</span>( F )           <span class="co"># Convert 1-by-1 data frame to simple number</span>
pvalue &lt;-<span class="st"> </span><span class="dv">2</span>*<span class="st"> </span>(<span class="dv">1</span>-<span class="kw">pf</span>( obs.F, <span class="dv">9</span>,<span class="dv">9</span> ))
pvalue</code></pre></div>
<pre><code>## [1] 0.4390223</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Using Rs built in function</span>
<span class="kw">var.test</span>( time ~<span class="st"> </span>trt, <span class="dt">data=</span>Alcohol )</code></pre></div>
<pre><code>## 
##  F test to compare two variances
## 
## data:  time by trt
## F = 1.7048, num df = 9, denom df = 9, p-value = 0.439
## alternative hypothesis: true ratio of variances is not equal to 1
## 95 percent confidence interval:
##  0.4234365 6.8633246
## sample estimates:
## ratio of variances 
##           1.704753</code></pre>
<p>We can wonder how well the theoretical estimate of the sampling distribution (F_{9,9}) compares to the simulation based estimate of the sampling distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Permutation distribution of Observed F-statistic assuming H0 is true. </span>
PermDist &lt;-<span class="st"> </span>mosaic::<span class="kw">do</span>(<span class="dv">10000</span>) *<span class="st"> </span>
<span class="st">  </span><span class="kw">var.test</span>(time ~<span class="st"> </span>mosaic::<span class="kw">shuffle</span>(trt), <span class="dt">data=</span>Alcohol)$statistic

<span class="co"># Figure which parts of the distribution are more extreme than my observed F </span>
PermDist &lt;-<span class="st"> </span>PermDist %&gt;%<span class="st">   </span>
<span class="st">  </span><span class="kw">mutate</span>( <span class="dt">extreme =</span> F &gt;<span class="st"> </span>obs.F |<span class="st"> </span>F &lt;<span class="st"> </span><span class="dv">1</span>/obs.F )</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Make a histogram of the permutation distribution along with the theoretical</span>
<span class="kw">ggplot</span>(PermDist, <span class="kw">aes</span>(<span class="dt">x=</span>F, <span class="dt">y=</span>..density..)) +
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth=</span>.<span class="dv">25</span>) +
<span class="st">  </span><span class="kw">geom_area</span>( <span class="dt">data=</span><span class="kw">data.frame</span>(<span class="dt">x=</span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dt">length=</span><span class="dv">1000</span>)) %&gt;%
<span class="st">                             </span><span class="kw">mutate</span>(<span class="dt">y=</span><span class="kw">df</span>(x, <span class="dv">9</span>,<span class="dv">9</span>)),
             <span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y), <span class="dt">alpha=</span>.<span class="dv">3</span>, <span class="dt">fill=</span><span class="st">&#39;red&#39;</span>)</code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-184-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># p-value... what percent is more extreme than what I observed?</span>
PermDist %&gt;%<span class="st"> </span><span class="kw">summarise</span>(<span class="dt">p.value =</span> <span class="kw">mean</span>(extreme))</code></pre></div>
<pre><code>##   p.value
## 1  0.6141</code></pre>
<p>The theoretical sampling distribution is more concentrated near 1 than the simulation estimate. As a result, the p-value is a bit larger, but in both cases, we cannot reject equal variances.</p>
<p><strong>Example</strong>: Lets consider a case where we have two groups of moderate sample sizes where there is a difference in variance. Suppose we consider the set of times in takes me to bike to work in the morning versus biking home. On the way to work, I get to go down Beaver street, but on the way home there is a lot of elevation gain. Also surprisingly often on the way home I run into other cyclists I know and we stop and chat or we end up riding some place neither of us has to go.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Commute &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">time =</span> <span class="kw">c</span>(<span class="fl">21.0</span>, <span class="fl">22.1</span>, <span class="fl">19.3</span>, <span class="fl">22.4</span>, <span class="fl">19.6</span>, <span class="fl">19.8</span>,
           <span class="fl">19.6</span>, <span class="fl">20.4</span>, <span class="fl">21.1</span>, <span class="fl">19.7</span>, <span class="fl">19.9</span>, <span class="fl">20.0</span>,
           <span class="fl">25.0</span>, <span class="fl">27.8</span>, <span class="fl">25.2</span>, <span class="fl">25.1</span>, <span class="fl">25.4</span>, <span class="fl">25.9</span>,
           <span class="fl">30.3</span>, <span class="fl">29.5</span>, <span class="fl">25.1</span>, <span class="fl">26.4</span>, <span class="fl">24.4</span>, <span class="fl">27.7</span>,
           <span class="fl">25.8</span>, <span class="fl">27.1</span>),
  <span class="dt">type =</span> <span class="kw">c</span>( <span class="kw">rep</span>(<span class="st">&#39;Morning&#39;</span>,<span class="dv">12</span>), <span class="kw">rep</span>(<span class="st">&#39;Evening&#39;</span>,<span class="dv">14</span>)))

<span class="kw">ggplot</span>(Commute, <span class="kw">aes</span>(<span class="dt">x=</span>type, <span class="dt">y=</span>time)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_boxplot</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title=</span><span class="st">&#39;Commute Times&#39;</span>, <span class="dt">y=</span><span class="st">&#39;Time (minutes)&#39;</span>, <span class="dt">x=</span><span class="st">&#39;Time of Day&#39;</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-186-1.png" width="672" /></p>
<p>We now test to see if there is a significant difference between the variances of these two groups. If we feel comfortable with assuming that these data come from normal distributions, then the theoretical method is appropriate.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var.test</span>( time ~<span class="st"> </span>type, <span class="dt">data=</span>Commute )</code></pre></div>
<pre><code>## 
##  F test to compare two variances
## 
## data:  time by type
## F = 3.039, num df = 13, denom df = 11, p-value = 0.07301
## alternative hypothesis: true ratio of variances is not equal to 1
## 95 percent confidence interval:
##  0.8959971 9.7171219
## sample estimates:
## ratio of variances 
##           3.038978</code></pre>
<p>But if we are uncomfortable with the normality assumption (the Shapiro-Wilks test indicates moderate evidence to reject normality for both samples due to the positive skew in both) we could compare our observed F-statistic to the simulation based estimate of the sampling distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># obs.F = 3.04</span>
obs.F &lt;-<span class="st"> </span><span class="kw">var.test</span>(time ~<span class="st"> </span>type, <span class="dt">data=</span>Commute)$statistic

<span class="co"># create the permutation distribution of F-values </span>
PermDist &lt;-<span class="st"> </span>mosaic::<span class="kw">do</span>(<span class="dv">10000</span>) *<span class="st"> </span>
<span class="st">  </span><span class="kw">var.test</span>(time ~<span class="st"> </span>mosaic::<span class="kw">shuffle</span>(type), <span class="dt">data=</span>Commute)$statistic

<span class="co"># Figure which parts of the distribution are more extreme than my observed F </span>
PermDist &lt;-<span class="st"> </span>PermDist %&gt;%<span class="st">   </span>
<span class="st">  </span><span class="kw">mutate</span>( <span class="dt">extreme =</span> F &gt;<span class="st"> </span>obs.F |<span class="st"> </span>F &lt;<span class="st"> </span><span class="dv">1</span>/obs.F )  <span class="co"># F &gt; 3.04 or F &lt; 1/3.04</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Make a histogram of the permutation distribution and theoretical</span>
<span class="kw">ggplot</span>(PermDist, <span class="kw">aes</span>(<span class="dt">x=</span>F, <span class="dt">y=</span>..density..)) +
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth=</span>.<span class="dv">1</span>) +
<span class="st">  </span><span class="kw">geom_area</span>( <span class="dt">data=</span><span class="kw">data.frame</span>(<span class="dt">x=</span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dt">length=</span><span class="dv">1000</span>)) %&gt;%
<span class="st">                             </span><span class="kw">mutate</span>(<span class="dt">y=</span><span class="kw">df</span>(x, <span class="dv">13</span>, <span class="dv">11</span>)),
             <span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y), <span class="dt">alpha=</span>.<span class="dv">3</span>, <span class="dt">fill=</span><span class="st">&#39;red&#39;</span>)</code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-189-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># p-value... what proportion is more extreme than what I observed?</span>
PermDist %&gt;%<span class="st"> </span><span class="kw">summarise</span>(<span class="dt">p.value =</span> <span class="kw">mean</span>(extreme))</code></pre></div>
<pre><code>##   p.value
## 1  0.0019</code></pre>
<p>We again see that with this small of a data set, our simulation based p-value is different from the theoretical based p-value. This is primarily due to the non-normality of our data along with the small sample sizes. In general as our sample sizes increase the simulation based and theoretical based distributions should give similar inference and p-values.</p>
</div>
<div id="exercises-7" class="section level2">
<h2><span class="header-section-number">8.5</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li><p>To find the probabilities for the F-distribution, we will use the function <code>pf(f, df1, df2)</code> where the <code>f</code> is the value for which we want to find the probability of finding a value less than. That is <span class="math display">\[P\left(F_{2,10}&lt;4.2\right)=\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pf</span>(<span class="fl">4.2</span>, <span class="dt">df1=</span><span class="dv">2</span>, <span class="dt">df2=</span><span class="dv">10</span>) </code></pre></div>
<pre><code>## [1] 0.9525855</code></pre>
<ol style="list-style-type: lower-alpha">
<li>Using the probability function for the F-distribution in R, find the following probabilities:
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(P\left(F_{5,5}&lt;\frac{1}{2}\right)\)</span></li>
<li><span class="math inline">\(P\left(F_{5,5}&gt;\frac{2}{1}\right)\)</span></li>
<li><span class="math inline">\(P\left(F_{4,10}&gt;\frac{6}{1}\right)\)</span></li>
<li><span class="math inline">\(P\left(F_{10,4}&lt;\frac{1}{6}\right)\)</span></li>
</ol></li>
<li>From what you calculated in part (a), comment on the reciprocal symmetry of the F-distribution.</li>
</ol></li>
<li>In this exercise we will examine the variability of samples from various distributions and how easily departures from normality are detected using qqplots and the Shapiro-Wilks test. Under no circumstances should you turn in page after page of output or graphs. Produce a table that summarizes how often the test rejects the null hypotheses and include at most one figure of QQ-plots. To receive credit, you must comment on the table and graph and describe what you observe and why you observed what you did.
<ol style="list-style-type: lower-alpha">
<li><p>The following code will create a random sample from a normal distribution and draw the qqplot. Also notice the results of the Shapiro-Wilks test. Investigate the behavior of repeated samples (ie run this code at least 10 times). Repeat with increased sample sizes (do this for n=5,25,100,400). Describe your results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)) <span class="co"># 1 row of 2 graphs, side-by-side</span>
n &lt;-<span class="st"> </span><span class="dv">5</span>                       <span class="co"># sample size is 5</span>
x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dt">mean=</span><span class="dv">25</span>, <span class="dt">sd=</span><span class="dv">5</span>) <span class="co"># draw random sample from a normal distribution</span>
<span class="kw">hist</span>(x)                      <span class="co"># histogram</span>
<span class="kw">qqnorm</span>(x)                    <span class="co"># qqplot for normality</span>
<span class="kw">qqline</span>(x)                    <span class="co"># add a line to the above plot</span>
<span class="kw">shapiro.test</span>(x)              <span class="co"># do the test for normality</span></code></pre></div></li>
<li><p>Repeat problem (a) but consider samples drawn from a distribution that is not normal, in this case, the gamma distribution with parameters <code>shape=3</code>, and <code>rate=2</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)) <span class="co"># 1 row of 2 graphs, side-by-side</span>
n &lt;-<span class="st"> </span><span class="dv">5</span>                          <span class="co"># sample size is 5</span>
x &lt;-<span class="st"> </span><span class="kw">rgamma</span>(n, <span class="dt">shape=</span><span class="dv">3</span>, <span class="dt">rate=</span><span class="dv">2</span>) <span class="co"># draw random sample from a gamma distribution</span>
<span class="kw">hist</span>(x)                         <span class="co"># histogram</span>
<span class="kw">qqnorm</span>(x)                       <span class="co"># qqplot for normality</span>
<span class="kw">qqline</span>(x)                       <span class="co"># add a line to the above plot</span>
<span class="kw">shapiro.test</span>(x)                 <span class="co"># do the test for normality</span></code></pre></div></li>
</ol></li>
<li><p>In this exercise, we will examine the variability of samples from a normal distribution. The following code will generate random samples from a normal distribution, create boxplots, and perform an F-test for equal variance. Run the code many times (again 20 or more times) and investigate the effects of changing your sample size and the mean and standard deviation of each group. <em>Under no circumstances should you turn in page after page of output or graphs. For each question produce a table that summarizes how often the test rejects the null hypotheses and include at most one figure of boxplots. To receive credit, you must comment on the table and graph and describe what you observe and why you observed what you did.</em></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>)) <span class="co"># 1 row of 1: Just one graph</span>
n &lt;-<span class="st"> </span><span class="dv">5</span> 
sigma &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>)   <span class="co"># Standard deviations of each group</span>
my.data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">y =</span> <span class="kw">c</span>(<span class="kw">rnorm</span>( n, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span>sigma[<span class="dv">1</span>] ),
                            <span class="kw">rnorm</span>( n, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span>sigma[<span class="dv">2</span>] )),
                      <span class="dt">group =</span> <span class="kw">c</span>( <span class="kw">rep</span>(<span class="st">&#39;g1&#39;</span>,n), <span class="kw">rep</span>(<span class="st">&#39;g2&#39;</span>,n)  ))
<span class="kw">boxplot</span>(y ~<span class="st"> </span>group, <span class="dt">data=</span>my.data) 
<span class="kw">var.test</span>(y ~<span class="st"> </span>group, <span class="dt">data=</span>my.data)</code></pre></div>
<ol style="list-style-type: lower-alpha">
<li>How often does the F-test reject (at <span class="math inline">\(\alpha=0.05\)</span> level) equal variance when the variances of the two groups are equal? (Run the above code 20 or more times.) Does this appear to change as the sample size gets larger?</li>
<li>How often does the F-test reject (at <span class="math inline">\(\alpha=0.05\)</span> level) equal variance when the variances are different, say <span class="math inline">\(\sigma_{1}=2\)</span> and <span class="math inline">\(\sigma_{2}=4\)</span>? <em>(Run your code many times!)</em></li>
<li>Is it surprising to you how much variability there is in the widths of the boxplots when the data are generated having the same standard deviation? With both groups having the same standard deviation, investigate the variability in boxplot widths as <span class="math inline">\(n\)</span> is <span class="math inline">\(5,20\)</span>, and <span class="math inline">\(50\)</span>.</li>
</ol></li>
<li><p>We are interested in testing if the variance is equal among two populations that are known to be normal. A sample of size <span class="math inline">\(n_{1}=15\)</span> from the first population resulted in a sample mean and standard deviation of <span class="math inline">\(\bar{x}_{1}=52\)</span> and <span class="math inline">\(s_{1}=7\)</span> while the sample of size <span class="math inline">\(n_{2}=20\)</span> from the second population had a sample mean and standard deviation of <span class="math inline">\(\bar{x}_{2}=42\)</span> and <span class="math inline">\(s_{2}=4\)</span>. Perform an F-test with <span class="math inline">\(\alpha=0.05\)</span> to test if the variances are different. <em>Because the data is not given, all calculations must be done by-hand, except the usual probability look up.</em></p></li>
<li><p>The life span of an electrical component was studied under two operating voltages (110 and 220). Ten components were randomly assigned to operate at 110 volts and 16 were assigned to 220 volts. The time to failure (in hundreds of hours) for the 26 components were obtained:</p>
<table>
<tbody>
<tr class="odd">
<td align="center"><strong>110</strong></td>
<td align="center"></td>
<td align="center">19.25</td>
<td align="center">19.7</td>
<td align="center">19.75</td>
<td align="center">19.9</td>
<td align="center">19.95</td>
<td align="center">20.05</td>
<td align="center">20.13</td>
<td align="center">20.2</td>
<td align="center">20.4</td>
<td align="center">20.6</td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center"><strong>220</strong></td>
<td align="center"></td>
<td align="center">9.7</td>
<td align="center">9.75</td>
<td align="center">9.8</td>
<td align="center">9.82</td>
<td align="center">9.85</td>
<td align="center">9.90</td>
<td align="center">9.92</td>
<td align="center">9.96</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="center"></td>
<td align="center">10.01</td>
<td align="center">10.02</td>
<td align="center">10.10</td>
<td align="center">10.11</td>
<td align="center">10.13</td>
<td align="center">10.19</td>
<td align="center">10.28</td>
<td align="center">10.31</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<ol style="list-style-type: lower-alpha">
<li>Calculate the mean and variance of each sample group</li>
<li>Test the assumption that the data in each group is normally distributed.
<ol style="list-style-type: lower-roman">
<li>Create the QQplots first and comment on their fit.</li>
<li>Perform the Shapiro-Wilks test to assess normality.</li>
</ol></li>
<li>Test the assumption that the variances in each group are equal
<ol style="list-style-type: lower-roman">
<li>By hand, perform a two-side hypothesis test that variances in each group are equal. <em>Here, “by hand” means to calculate the f-statistic by hand and then form the probability statement that defines the p-value. Then use the pf() function to calculate the actual p-value.</em></li>
<li>Using the R function <code>var.test()</code> confirm your calculations in part (ii).</li>
</ol></li>
</ol></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="9-analysis-of-variance-anova.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dereksonderegger/570/raw/master/08_TestingAssumptions.Rmd",
"text": "Edit"
},
"download": [["Statistical_Methods_I.pdf", "PDF"], ["Statistical_Methods_I.epub", "EPUB"]],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
